{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J0yGl14VQIj"
      },
      "source": [
        "# **Env. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEOC_PiVVjx"
      },
      "source": [
        "pip installation: Colab TPU\n",
        "Colab TPU runtimes come with JAX pre-installed, but before importing JAX you must run the following code to initialize the TPU (we added these lines to our finetuning code already). This code should be part of your python file not in the colab cell :\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "Colab TPU runtimes use an older TPU architecture than Cloud TPU VMs, so installing jax[tpu] should be avoided on Colab. If for any reason you would like to update the jax & jaxlib libraries on a Colab TPU runtime, follow the CPU instructions above (i.e. install jax[cpu]).\n",
        "\n",
        "https://github.com/google/jax#pip-installation-colab-tpu\n",
        "\n",
        "FLAX (JAX) is also flexible to work with GPU and to make it work with GPU it is better to remove the below lines from each code. You will find these lines at the beginning of each finetuning code in our examples.\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note here that if you got any error related to Jax lib or FLAX, then you should roll back to the earlier version of flax from this list below:\n",
        "\n",
        "\n",
        "https://pypi.org/project/flax/#history"
      ],
      "metadata": {
        "id": "yRpglxhDf2Iq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbCTAfpnYLCr",
        "outputId": "c66564c4-52a6-4766-9067-b0d3378b5ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (1.24.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.3 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.24.2)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.24.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from farasapy) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from farasapy) (4.65.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->farasapy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->farasapy) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->farasapy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->farasapy) (3.4)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzysearch\n",
            "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.9/dist-packages (from fuzzysearch) (22.2.0)\n",
            "Building wheels for collected packages: fuzzysearch\n",
            "  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp39-cp39-linux_x86_64.whl size=355117 sha256=05045ec70963abf18fa065fb594f0c724341fea37d90957914c25f162f6addbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/29/ab/55ae4e35024221de1fc9f14d4fafe1ba9dc461623225e120ec\n",
            "Successfully built fuzzysearch\n",
            "Installing collected packages: fuzzysearch\n",
            "Successfully installed fuzzysearch-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from bert_score) (4.26.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from bert_score) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bert_score) (1.24.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from bert_score) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.9/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from bert_score) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->bert_score) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.24.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=21f646458a968651d364758c229bd54947bc1a7daa7b38fddba7e5d56b424599\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting camel_tools\n",
            "  Downloading camel_tools-1.5.2-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from camel_tools) (1.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from camel_tools) (4.26.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting camel-kenlm>=2023.3.17.2\n",
            "  Downloading camel-kenlm-2023.3.17.2.tar.gz (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.6/426.6 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.9/dist-packages (from camel_tools) (0.6.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.9/dist-packages (from camel_tools) (2.0.0+cu118)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from camel_tools) (0.18.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from camel_tools) (2.27.1)\n",
            "Collecting muddler\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from camel_tools) (0.3.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from camel_tools) (5.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from camel_tools) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.9/dist-packages (from camel_tools) (0.19.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from camel_tools) (1.24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from camel_tools) (1.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from camel_tools) (0.8.10)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from camel_tools) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from camel_tools) (4.65.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->camel_tools) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->camel_tools) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->camel_tools) (16.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.2->camel_tools) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.2->camel_tools) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.2->camel_tools) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.2->camel_tools) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.2->camel_tools) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->camel_tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->camel_tools) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->camel_tools) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->camel_tools) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->camel_tools) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->camel_tools) (2.0.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->camel_tools) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->camel_tools) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3->camel_tools) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3->camel_tools) (1.3.0)\n",
            "Building wheels for collected packages: camel-kenlm, docopt, emoji\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2023.3.17.2-cp39-cp39-linux_x86_64.whl size=3543851 sha256=5e6e52ab842e25755c8325a80e41bda524019b49e92c3cb38ecf8b51be2a5cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/c6/e6/1943f2f26acf2172c51431e3e0a37ef1f588bdcf9a2ba476e8\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=4e62631f0b2cc92bb266e5a529e02ff755015e39e094c63933fd9194925c9189\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=8fad60c5af260591a0c1448599ecac378d68fa9ff04ad464e5eb13b6cf79bad6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built camel-kenlm docopt emoji\n",
            "Installing collected packages: docopt, camel-kenlm, muddler, emoji, camel_tools\n",
            "Successfully installed camel-kenlm-2023.3.17.2 camel_tools-1.5.2 docopt-0.6.2 emoji-2.2.0 muddler-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers==4.26.1\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip3 install evaluate\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install fuzzysearch\n",
        "!pip3 install bert_score\n",
        "!pip3 install rouge_score\n",
        "!pip3 install camel_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaxlib and Jax of Google Colab use a special version of Jaxlib that work with google colab. A newer version of jaxlib may only work with TPU VM devices. So first, we need to check what Jax and Jaxlib versions this Google colab uses, and then we will force the installation of Flax to match both the current Jax and Jaxlib versions."
      ],
      "metadata": {
        "id": "k67YDTYYjc5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "import jax\n",
        "import jaxlib\n",
        "print(jax.__version__)\n",
        "print(jaxlib.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErmTwuXejY3C",
        "outputId": "c8742ec6-ae96-4a51-fe13-e86083a42f21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.25\n",
            "0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax==0.3.25 jaxlib==0.3.25 flax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8-gXGUghmvo",
        "outputId": "275ed822-60c7-489a-bc23-2fcfdf8777be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax==0.3.25 in /usr/local/lib/python3.9/dist-packages (0.3.25)\n",
            "Requirement already satisfied: jaxlib==0.3.25 in /usr/local/lib/python3.9/dist-packages (0.3.25)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.9/dist-packages (0.6.8)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax==0.3.25) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax==0.3.25) (1.24.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from jax==0.3.25) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax==0.3.25) (1.10.1)\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.7-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.2/214.2 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.6-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.4-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorstore in /usr/local/lib/python3.9/dist-packages (from flax) (0.1.35)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.9/dist-packages (from flax) (13.3.3)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (from flax) (0.1.4)\n",
            "Requirement already satisfied: orbax in /usr/local/lib/python3.9/dist-packages (from flax) (0.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from flax) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from flax) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from flax) (1.0.5)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax) (2.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->flax) (4.39.3)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax->flax) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax->flax) (1.4.0)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.6-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.5-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.4-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: etils in /usr/local/lib/python3.9/dist-packages (from orbax->flax) (1.1.1)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.9/dist-packages (from orbax->flax) (1.5.2)\n",
            "  Downloading orbax-0.1.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->flax) (3.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->flax) (1.16.0)\n",
            "Installing collected packages: chex, orbax, flax\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.7\n",
            "    Uninstalling chex-0.1.7:\n",
            "      Successfully uninstalled chex-0.1.7\n",
            "  Attempting uninstall: orbax\n",
            "    Found existing installation: orbax 0.1.7\n",
            "    Uninstalling orbax-0.1.7:\n",
            "      Successfully uninstalled orbax-0.1.7\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.6.8\n",
            "    Uninstalling flax-0.6.8:\n",
            "      Successfully uninstalled flax-0.6.8\n",
            "Successfully installed chex-0.1.6 flax-0.6.4 orbax-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQ02AciJkGF"
      },
      "source": [
        "# **General Direction in how to use FLAX and T5 Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwsJKblDnV0-"
      },
      "source": [
        "Seq2Seq model work simply by having an input text and target text and you add tags for each one. In contrast to extractive models like BERT, ELECTRA, and ALBERT, we use the same input-target format for all tasks. In this colab, we will adapt this code https://github.com/huggingface/transformers/blob/main/examples/flax/summarization/run_summarization_flax.py . If you go down, you will find code called \"T5 QA\" which is an adapation of summarization code for QA tasks.\n",
        "\n",
        "For example:\n",
        "- let's say that you try to target machine translation and you have a CSV file that has the first column \"english_text\" and a second column \"arabic_text\", then your code will look like this  :\n",
        "``` \n",
        "for english_text,arabic_text in zip(examples[\"english_text\"],examples[\"arabic_text\"]):\n",
        "            inputs.append(\"english : %s\" % (english_text))\n",
        "            targets.append(\"arabic : %s\" % (arabic_text))\n",
        "```\n",
        "after the code will read all examples, your input, and output will look like this :\n",
        "```\n",
        "input: <english : artificial intelligence is the future>\n",
        "target: <arabic : الذكاء الاصطناعي هو المستقبل >\n",
        "```\n",
        "We use the zip function to combine two entries and we use %s to insert variables that come after % mark to our text. If you have two entries (e.g, in question answering task we have both context and question as input), then you can have something like this :\n",
        "```\n",
        "for context,question,answer in zip(examples[\"context\"],examples[\"question\"],examples['answers']):\n",
        "            inputs.append(\"question: %s context: %s\" % (question,context))\n",
        "            targets.append(\"answer: %s\" % (answer[\"text\"]))\n",
        "```\n",
        "Your example after preprocessing will look like this :\n",
        "```\n",
        " input: <question: ماهي عاصمة ايطاليا context : روما هي عاصمة ايطاليا واكبر مدنها >\n",
        " target: < روما >\n",
        "```\n",
        "- For Sentiment Analysis :\n",
        "Assuming you have a column named \"sentence\" and \"labels\" in your CSV files, then you can have a code like this \n",
        "```\n",
        "for sentence,label in zip(examples[\"sentence\"],examples[\"label\"]):\n",
        "            inputs.append(\"%s\" % (sentence))\n",
        "            targets.append(\"%s\" % (label))\n",
        "```\n",
        "Your example after preprocessing will look like this:\n",
        "```\n",
        " input: <التلفاز الذي قمت بشراءه من المتجر لم يعجبني >\n",
        " target: < negative >\n",
        "```\n",
        "\n",
        "- For Text Summarization :\n",
        "Assuming you have a column named \"article\" and \"summary\" in your CSV files, then you can have a code like this \n",
        "```\n",
        "for article,summary in zip(examples[\"article\"],examples[\"summary\"]):\n",
        "            inputs.append(\"%s\" % (article))\n",
        "            targets.append(\"%s\" % (summary))\n",
        "```\n",
        "\n",
        "\n",
        "Observe also that we did not use prefix in examples where we have one entry but we need it when our example consists of two entries like question answering. \n",
        "\n",
        "\n",
        "You also need to prefix even if your example has one entry if you planning to finetune the T5 model to target different tasks. For example, if you fine-tune your model in question answering first and you add a prefix in the input and target and then you finetune it in sentiment analysis by adding a prefix also. The model then will understand which task you are targeting during the inference by detecting those prefixes.\n",
        "\n",
        "If you want to use a different evaluation metric, then you can add your evaluation metric method (e.g F1, Accuracy .. ) in lines 765-775 under  compute_metrics function. \n",
        "\n",
        "Huggingface evaluate library has a list of evaluation metrics that you can load using evaluate.load(\"\") function.\n",
        "\n",
        "For example to add a function for EM/F1 score for Arabic TyDi Evaluation, we first add these lines to line 605 (this EM/F1 function is adapted from this [colab](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def normalize_answer(s):\n",
        "   \n",
        "      def remove_articles(text):\n",
        "          return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "      def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "\n",
        "      def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "      def lower(text):\n",
        "          return text.lower()\n",
        "\n",
        "      def ar_normalize(text):\n",
        "          text = text.strip()\n",
        "          text = normalize_alef_ar(text)\n",
        "          text = normalize_alef_maksura_ar(text)\n",
        "          text = normalize_teh_marbuta_ar(text)\n",
        "          text=  araby.strip_diacritics(text)\n",
        "          return text\n",
        "      \n",
        "      return ar_normalize(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
        "\n",
        "\n",
        "    def f1_score(prediction, ground_truth):\n",
        "        prediction_tokens = normalize_answer(prediction).split()\n",
        "        ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "\n",
        "    def exact_match_score(prediction, ground_truth):\n",
        "        return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "    def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "        scores_for_ground_truths = []\n",
        "        for ground_truth in ground_truths:\n",
        "            score = metric_fn(prediction, ground_truth)\n",
        "            scores_for_ground_truths.append(score)\n",
        "        return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "    def evaluate_qa(gold_answers, predictions):\n",
        "        f1 = exact_match = total = 0\n",
        "\n",
        "        for ground_truths, prediction in zip(gold_answers, predictions):\n",
        "          total += 1\n",
        "          exact_match += metric_max_over_ground_truths(\n",
        "                        exact_match_score, prediction, ground_truths)\n",
        "          f1 += metric_max_over_ground_truths(\n",
        "              f1_score, prediction, ground_truths)\n",
        "        \n",
        "        exact_match = exact_match / total\n",
        "        f1 = f1 / total\n",
        "        return exact_match,f1\n",
        "```\n",
        "\n",
        "Then, we use this evaluation function by reporting its results in lines 709 :\n",
        "\n",
        "\n",
        "```\n",
        " result[\"EM\"],result[\"F1\"]=evaluate_qa(references,decoded_preds)\n",
        " ```\n",
        "\n",
        "Basicly, you need to insert any evaluation score you want the code to show it up during the evaluation to [result] dictionary inside compute_metrics function.\n",
        "\n",
        "Also, observe that in line 69 we add this code :\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "You can remove it if you are using GPU.\n",
        "\n",
        "Finally, if you prefer to work with torch code instead of FLAX, you can do the same change we did it here to https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py . Both FLAX and Torch code are very similar except for small change to fit FLAX environment ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ops9-aMOVK5m"
      },
      "source": [
        "# **Running Flax Code on Question Answering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIu8ZvqJ0_7a"
      },
      "source": [
        "First lets process Arabic TyDi QA dataset. TyDi combine all questions for more than 11 different language so we need to extract the arabic portion and also do some pre-processing. All credits to AUB Mind LAB mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oefGTYcdoZpZ",
        "outputId": "f5257e6c-a08c-4917-9a03-4bd06ebd653b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 28.54 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aub-mind/arabert\n",
        "!cp arabert/examples/question-answering/utils_qa.py .\n",
        "!cp arabert/examples/question-answering/trainer_qa.py .\n",
        "!cp arabert/examples/question-answering/run_qa.py .\n",
        "!cp arabert/examples/question-answering/squad_preprocessing.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrRzVWNocBh",
        "outputId": "6739d5f3-220f-4317-a457-bc0321729fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 18:00:40--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.200.128, 209.85.234.128, 142.250.152.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.200.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58004076 (55M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-train.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-t 100%[===================>]  55.32M   108MB/s    in 0.5s    \n",
            "\n",
            "2023-04-05 18:00:40 (108 MB/s) - ‘tydiqa-goldp-v1.1-train.json’ saved [58004076/58004076]\n",
            "\n",
            "--2023-04-05 18:00:40--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.200.128, 209.85.234.128, 142.250.152.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.200.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5617409 (5.4M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-dev.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-d 100%[===================>]   5.36M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-04-05 18:00:41 (99.8 MB/s) - ‘tydiqa-goldp-v1.1-dev.json’ saved [5617409/5617409]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mGIdA3eJoeRZ"
      },
      "outputs": [],
      "source": [
        "model_name=\"sultan/ArabicT5-Large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68MU-v3ogL7",
        "outputId": "342f7abd-79cf-4cc6-dab0-0231a120d726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 18:00:46.019302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "sultan/ArabicT5-Large\n",
            "W0405 18:00:47.837744 139741438805824 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            " 38% 18948/49881 [00:00<00:00, 187454.26it/s]WARNING:tensorflow:Could not find answer for question 'arabic-1804271180688859213-0' :\n",
            " 'الإسكندر الثالث المقدوني ، المعروف بأسماء عديدة أخرى أبرزها : الإسكندر الأكبر ، و < b data - parsoid = ' { \" dsr \" : [1849 , 1870 , 3 , 3] } ' > الإسكندر الكبير ، و < b data - parsoid = ' { \" dsr \" : [1873 , 1896 , 3 , 3] } ' > الإسكندر المقدوني ، و < b data - parsoid = ' { \" dsr \" : [1899 , 1924 , 3 , 3] } ' > الإسكندر ذو القرنين ( باليونانية : ؛ نقحرة : ) ، هو أحد ملوك مقدونيا الإغريق ، ومن أشهر القادة العسكريين والفاتحين عبر التاريخ . ولد الإسكندر في مدينة يلا قرابة سنة 356 ق . م ، وتتلمذ على يد الفيلسوف والعالم الشهير أرسطو حتى بلغ ربيعه السادس عشر . وبحلول عامه الثلاثين ، كان قد أسس إحدى أكبر وأعظم الإمبراطوريات التي عرفها العالم القديم ، والتي امتدت من سواحل البحر الأيوني غربا وصولا إلى سلسلة جبال الهيمالايا شرقا . يعد أحد أنجح القادة العسكريين في مسيرتهم ، إذ لم يحصل أن هزم في أي معركة خاضها على الإطلاق . [1]' \n",
            "vs.\n",
            " 'Ἀλέξανδρο'\n",
            "orig answer:\n",
            " 'Ἀλέξανδρο'\n",
            "==================\n",
            "100% 49881/49881 [00:17<00:00, 2815.96it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 1 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-train.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-train-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhP2gAHohvr",
        "outputId": "f36af44a-161b-4284-da57-f194a65b2227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 18:01:13.648047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "sultan/ArabicT5-Large\n",
            "W0405 18:01:15.313910 140408257574720 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            "100% 5077/5077 [00:01<00:00, 4604.96it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 0 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-dev.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-dev-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vsNd92609mxl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "def squad_json_to_csv(json_file,csv_output):\n",
        " with open(csv_output,  'w', newline='') as csvfile:\n",
        "    tsv_writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
        "    with open(json_file, 'r') as tydi_json:\n",
        "      data = json.loads(tydi_json.read())\n",
        "      tydi=data[\"data\"]\n",
        "      tsv_writer.writerow([\"context\",\"title\",\"question\",\"answer1\",\"answer2\"])\n",
        "      for entry in tydi:\n",
        "        if \"arabic\" in entry[\"paragraphs\"][0][\"qas\"][0][\"id\"]:\n",
        "            answers=[] ### in some question of TyDi there is more than one answer (ground truths) for the question.\n",
        "            for answer in entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"]:\n",
        "             answers.append(answer[\"text\"])\n",
        "            if len(answers)==1: ### if there is one answer then add the first one to answer1 column and empty \"\" value to answer2\n",
        "             tsv_writer.writerow([entry[\"paragraphs\"][0][\"context\"],entry[\"title\"],entry[\"paragraphs\"][0][\"qas\"][0][\"question\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"],\"-\"])\n",
        "            else: ## if there are two answers, add the first one to answer1 column and second one to answer2 column\n",
        "             tsv_writer.writerow([entry[\"paragraphs\"][0][\"context\"],entry[\"title\"],entry[\"paragraphs\"][0][\"qas\"][0][\"question\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][1][\"text\"]])\n",
        "squad_json_to_csv(\"tydiqa-goldp-v1.1-train-pre.json\",\"tydiqa-goldp-v1.1-train-pre.csv\")\n",
        "squad_json_to_csv(\"tydiqa-goldp-v1.1-dev-pre.json\",\"tydiqa-goldp-v1.1-dev-pre.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiXpjSg9PEAx"
      },
      "source": [
        "Below we use the code \"https://github.com/huggingface/transformers/tree/main/examples/flax/summarization\" at the huggingface library and we modified it for our task. You should click on the play button besides \"show code\" so the code will be written to this colab. If you want to use this code in your local machine which has a gpu, you can simply save this written file to your local machine and run the code.\n",
        "\n",
        "make sure to remove the line\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "if you are planning to use your GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzDOj2pcQ6Q",
        "outputId": "4609ac58-0292-4987-ec2b-a1e77d8cc111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting t5_qa.py\n"
          ]
        }
      ],
      "source": [
        "#@title T5 QA\n",
        "%%writefile t5_qa.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "import pyarabic.araby as araby\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "    def normalize_answer(s):\n",
        "   \n",
        "      def remove_articles(text):\n",
        "          return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "      def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "\n",
        "      def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "      def lower(text):\n",
        "          return text.lower()\n",
        "\n",
        "      def ar_normalize(text):\n",
        "          text = text.strip()\n",
        "          text = normalize_alef_ar(text)\n",
        "          text = normalize_alef_maksura_ar(text)\n",
        "          text = normalize_teh_marbuta_ar(text)\n",
        "          text=  araby.strip_diacritics(text)\n",
        "          return text\n",
        "      \n",
        "      return ar_normalize(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
        "\n",
        "\n",
        "    def f1_score(prediction, ground_truth):\n",
        "        prediction_tokens = normalize_answer(prediction).split()\n",
        "        ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "\n",
        "    def exact_match_score(prediction, ground_truth):\n",
        "        return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "    def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "        scores_for_ground_truths = []\n",
        "        for ground_truth in ground_truths:\n",
        "            score = metric_fn(prediction, ground_truth)\n",
        "            scores_for_ground_truths.append(score)\n",
        "        return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "    def evaluate_qa(gold_answers, predictions):\n",
        "        f1 = exact_match = total = 0\n",
        "\n",
        "        for ground_truths, prediction in zip(gold_answers, predictions):\n",
        "          total += 1\n",
        "          exact_match += metric_max_over_ground_truths(\n",
        "                        exact_match_score, prediction, ground_truths)\n",
        "          f1 += metric_max_over_ground_truths(\n",
        "              f1_score, prediction, ground_truths)\n",
        "        \n",
        "        exact_match = exact_match / total\n",
        "        f1 = f1 / total\n",
        "        return exact_match,f1\n",
        "\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        #inputs = examples[\"context_question\"]\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "        #### Question Answering ####\n",
        "        ## although for some question in TyDi we have more than one answer, we will only use first one for training. For evaluation we will use both.\n",
        "        for context,question,answer in zip(examples[\"context\"],examples[\"question\"],examples['answer1']):\n",
        "            inputs.append(\"question: %s context: %s\" % (question,context))\n",
        "            targets.append(\"%s\" % (answer))\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    \n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [norm_seq(pred.strip()) for pred in preds]\n",
        "        labels = [norm_seq(label.strip()) for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        ### add below lines if you want to calcuate rouge results #####\n",
        "        #metric = evaluate.load(\"rouge\")\n",
        "        #result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "        ##############################\n",
        "        #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        #result[\"BLEU\"]=evaluate.load(\"bleu\").compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
        "        #result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        #### this code block for TyDi evaluation. Remove it if you have different task ###\n",
        "        result={}\n",
        "        tydi_dev_file=pd.read_csv(data_args.validation_file)\n",
        "        references=[]\n",
        "        for index,row in tydi_dev_file.iterrows():\n",
        "          answers=[]\n",
        "          answers.append(str(row[\"answer1\"]))\n",
        "          if str(row[\"answer2\"])!=\"\":\n",
        "            answers.append(str(row[\"answer2\"]))\n",
        "          references.append(answers)\n",
        "        result[\"EM\"],result[\"F1\"]=evaluate_qa(references,decoded_preds)\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkSaTX0oalvh"
      },
      "source": [
        "Here we choose larger batch size to make our training faster but you can reduce the batch size for better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaVWsUF8IP15",
        "outputId": "67119ea0-e20b-4996-c223-b0e62c5ccd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 18:43:30.557707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=10.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-b4e3cc422429338f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 2/2 [00:00<00:00, 588.63it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('decoder', 'embed_tokens', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('lm_head', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 14805\n",
            "INFO:__main__:  Num Epochs = 10\n",
            "INFO:__main__:  Instantaneous batch size per device = 4\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 32\n",
            "INFO:__main__:  Total optimization steps = 4620\n",
            "Training...: 100% 462/462 [07:03<00:00,  1.09it/s]\n",
            "Epoch... (1/10 | Loss: 1.1056472063064575, Learning Rate: 9.002164733828977e-05)\n",
            "Evaluating...:   0% 0/15 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 15/15 [01:41<00:00,  6.78s/it]\n",
            "Epoch... (1/10 | Eval Loss: 0.5688 | Eval EM: 46.9055 | Eval F1: 68.5417 |)\n",
            "Training...: 100% 462/462 [04:13<00:00,  1.82it/s]\n",
            "Epoch... (2/10 | Loss: 0.7387481331825256, Learning Rate: 8.002164395293221e-05)\n",
            "Evaluating...: 100% 15/15 [00:11<00:00,  1.34it/s]\n",
            "Epoch... (2/10 | Eval Loss: 0.4802 | Eval EM: 56.3518 | Eval F1: 74.6228 |)\n",
            "Training...: 100% 462/462 [04:13<00:00,  1.82it/s]\n",
            "Epoch... (3/10 | Loss: 0.8324463367462158, Learning Rate: 7.002164056757465e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.41it/s]\n",
            "Epoch... (3/10 | Eval Loss: 0.4124 | Eval EM: 60.8035 | Eval F1: 77.5557 |)\n",
            "Training...: 100% 462/462 [04:13<00:00,  1.82it/s]\n",
            "Epoch... (4/10 | Loss: 0.7153145670890808, Learning Rate: 6.0021644458174706e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.38it/s]\n",
            "Epoch... (4/10 | Eval Loss: 0.3932 | Eval EM: 61.6721 | Eval F1: 79.1449 |)\n",
            "Training...: 100% 462/462 [04:12<00:00,  1.83it/s]\n",
            "Epoch... (5/10 | Loss: 0.5265498161315918, Learning Rate: 5.002164107281715e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.40it/s]\n",
            "Epoch... (5/10 | Eval Loss: 0.3836 | Eval EM: 64.6037 | Eval F1: 80.7329 |)\n",
            "Training...: 100% 462/462 [04:11<00:00,  1.83it/s]\n",
            "Epoch... (6/10 | Loss: 0.4194461703300476, Learning Rate: 4.00216449634172e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.39it/s]\n",
            "Epoch... (6/10 | Eval Loss: 0.3676 | Eval EM: 65.038 | Eval F1: 81.3214 |)\n",
            "Training...: 100% 462/462 [04:12<00:00,  1.83it/s]\n",
            "Epoch... (7/10 | Loss: 0.5360310673713684, Learning Rate: 3.0021643397049047e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.40it/s]\n",
            "Epoch... (7/10 | Eval Loss: 0.3565 | Eval EM: 66.6667 | Eval F1: 82.0376 |)\n",
            "Training...: 100% 462/462 [04:12<00:00,  1.83it/s]\n",
            "Epoch... (8/10 | Loss: 0.587415337562561, Learning Rate: 2.002164001169149e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.39it/s]\n",
            "Epoch... (8/10 | Eval Loss: 0.3554 | Eval EM: 66.4495 | Eval F1: 82.1031 |)\n",
            "Training...: 100% 462/462 [04:14<00:00,  1.82it/s]\n",
            "Epoch... (9/10 | Loss: 0.8387949466705322, Learning Rate: 1.0021644811786246e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.41it/s]\n",
            "Epoch... (9/10 | Eval Loss: 0.3544 | Eval EM: 66.3409 | Eval F1: 82.104 |)\n",
            "Training...:  45% 206/462 [01:53<02:24,  1.77it/s]"
          ]
        }
      ],
      "source": [
        "!python3 t5_qa.py \\\n",
        "\t--output_dir out \\\n",
        "\t--model_name_or_path sultan/ArabicT5-Base \\\n",
        "\t--tokenizer_name sultan/ArabicT5-Base \\\n",
        "\t--train_file=\"tydiqa-goldp-v1.1-train-pre.csv\" \\\n",
        "\t--validation_file=\"tydiqa-goldp-v1.1-dev-pre.csv\" \\\n",
        "\t--do_train --do_eval --predict_with_generate \\\n",
        "\t--num_train_epochs 10 \\\n",
        "\t--overwrite_cache \\\n",
        "\t--learning_rate 1e-4 --warmup_steps 0 \\\n",
        "\t--per_device_train_batch_size 4 \\\n",
        "\t--per_device_eval_batch_size 8 \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--max_source_length 512 --max_target_length 128 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8l6Uh0Hov5A"
      },
      "source": [
        "The code also will produce predictions and save it to \"pred.csv\" file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEvuvq7XTU8p"
      },
      "source": [
        "# **Running Flax Code on Text Summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IICQMDn6UUhS"
      },
      "source": [
        "Here we did small change to Question Answering FLAX code in line 665 by having this code :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for document,title in zip(examples[\"document\"],examples[\"title\"]):\n",
        "            inputs.append(\"<s> %s\" % (document))\n",
        "            targets.append(\"<s> %s\" % (title))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qfildV0TedE",
        "outputId": "ae827820-ca8f-4a16-a005-27358b29ce46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing t5_summary.py\n"
          ]
        }
      ],
      "source": [
        "#@title T5 FLAX Summarization Code\n",
        "%%writefile t5_summary.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        #inputs = examples[\"context_question\"]\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "\n",
        "        for document,title in zip(examples[\"document\"],examples[\"title\"]):\n",
        "            inputs.append(\"%s\" % (document))\n",
        "            targets.append(\"%s\" % (title))\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    \n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    metric = evaluate.load(\"rouge\")\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [norm_seq(pred.strip()) for pred in preds]\n",
        "        labels = [norm_seq(label.strip()) for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result[\"BLEU\"]=evaluate.load(\"bleu\").compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
        "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuiEhpVC1j3M"
      },
      "source": [
        "Lets try a sample dataset this link : https://github.com/UBC-NLP/araT5 . all credits to AraT5 team."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P8qLEaYy22_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c92326-e385-4ea4-b715-20cb45e071f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 19:48:13--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/ARGEn_title_genration_sample_valid.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 288031 (281K) [text/plain]\n",
            "Saving to: ‘ARGEn_title_genration_sample_valid.tsv’\n",
            "\n",
            "ARGEn_title_genrati 100%[===================>] 281.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-04-05 19:48:13 (8.28 MB/s) - ‘ARGEn_title_genration_sample_valid.tsv’ saved [288031/288031]\n",
            "\n",
            "--2023-04-05 19:48:13--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/ARGEn_title_genration_sample_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2961751 (2.8M) [text/plain]\n",
            "Saving to: ‘ARGEn_title_genration_sample_train.tsv’\n",
            "\n",
            "ARGEn_title_genrati 100%[===================>]   2.82M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-04-05 19:48:13 (42.0 MB/s) - ‘ARGEn_title_genration_sample_train.tsv’ saved [2961751/2961751]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/ARGEn_title_genration_sample_valid.tsv\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/ARGEn_title_genration_sample_train.tsv\n",
        "pd.read_table(\"ARGEn_title_genration_sample_train.tsv\",sep='\\t').to_csv('ARGEn_title_genration_sample_train.csv',index=False)\n",
        "pd.read_table(\"ARGEn_title_genration_sample_valid.tsv\",sep='\\t').to_csv('ARGEn_title_genration_sample_valid.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq68drV42x6W",
        "outputId": "bfd08d59-686a-419c-d752-3677be198ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 19:48:20.725169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=20.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-d30b8253e82fa9c0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 9137.92it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1311.74it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d30b8253e82fa9c0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 751.53it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('lm_head', 'kernel'), ('decoder', 'embed_tokens', 'kernel'), ('encoder', 'embed_tokens', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Downloading builder script: 6.27kB [00:00, 2.73MB/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 1000\n",
            "INFO:__main__:  Num Epochs = 20\n",
            "INFO:__main__:  Instantaneous batch size per device = 4\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 32\n",
            "INFO:__main__:  Total optimization steps = 620\n",
            "Training...: 100% 31/31 [03:12<00:00,  6.20s/it]\n",
            "Epoch... (1/20 | Loss: 5.139512538909912, Learning Rate: 9.516128920949996e-05)\n",
            "Evaluating...:   0% 0/4 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 4/4 [01:31<00:00, 22.88s/it]\n",
            "\n",
            "Downloading builder script: 5.94kB [00:00, 4.03MB/s]\n",
            "\n",
            "Downloading extra modules: 4.07kB [00:00, 2.80MB/s]       \n",
            "\n",
            "Downloading extra modules: 3.34kB [00:00, 2.56MB/s]\n",
            "Epoch... (1/20 | Eval Loss: 4.1189 | Eval rouge1: 2.5 | Eval rouge2: 0.0 | Eval rougeL: 2.5 | Eval rougeLsum: 2.3333 | Eval BLEU: 4.1551 | Eval gen_len: 1021.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.86it/s]\n",
            "Epoch... (2/20 | Loss: 4.250323295593262, Learning Rate: 9.016128751682118e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.72it/s]\n",
            "Epoch... (2/20 | Eval Loss: 3.8305 | Eval rouge1: 7.2833 | Eval rouge2: 1.0 | Eval rougeL: 7.3333 | Eval rougeLsum: 7.2333 | Eval BLEU: 6.9949 | Eval gen_len: 1272.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.90it/s]\n",
            "Epoch... (3/20 | Loss: 4.26509428024292, Learning Rate: 8.516129310010001e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.68it/s]\n",
            "Epoch... (3/20 | Eval Loss: 3.5965 | Eval rouge1: 7.8 | Eval rouge2: 2.0 | Eval rougeL: 7.8 | Eval rougeLsum: 7.6667 | Eval BLEU: 7.65 | Eval gen_len: 1292.0 |)\n",
            "Training...: 100% 31/31 [00:17<00:00,  1.80it/s]\n",
            "Epoch... (4/20 | Loss: 3.861194372177124, Learning Rate: 8.016129140742123e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.74it/s]\n",
            "Epoch... (4/20 | Eval Loss: 3.3713 | Eval rouge1: 8.6333 | Eval rouge2: 2.1429 | Eval rougeL: 8.6333 | Eval rougeLsum: 8.6 | Eval BLEU: 8.3251 | Eval gen_len: 1403.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.83it/s]\n",
            "Epoch... (5/20 | Loss: 3.5068628787994385, Learning Rate: 7.516128971474245e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.67it/s]\n",
            "Epoch... (5/20 | Eval Loss: 3.1017 | Eval rouge1: 7.0476 | Eval rouge2: 0.8 | Eval rougeL: 7.0667 | Eval rougeLsum: 7.081 | Eval BLEU: 8.1302 | Eval gen_len: 1271.0 |)\n",
            "Training...: 100% 31/31 [00:17<00:00,  1.82it/s]\n",
            "Epoch... (6/20 | Loss: 3.255988597869873, Learning Rate: 7.016129529802129e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.83it/s]\n",
            "Epoch... (6/20 | Eval Loss: 2.9859 | Eval rouge1: 6.8667 | Eval rouge2: 0.8 | Eval rougeL: 6.919 | Eval rougeLsum: 6.7857 | Eval BLEU: 9.0135 | Eval gen_len: 1411.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.87it/s]\n",
            "Epoch... (7/20 | Loss: 3.157200574874878, Learning Rate: 6.516128632938489e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.68it/s]\n",
            "Epoch... (7/20 | Eval Loss: 2.8813 | Eval rouge1: 7.0 | Eval rouge2: 1.0 | Eval rougeL: 7.0 | Eval rougeLsum: 7.0 | Eval BLEU: 8.6927 | Eval gen_len: 1392.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.87it/s]\n",
            "Epoch... (8/20 | Loss: 2.783205032348633, Learning Rate: 6.016129191266373e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.75it/s]\n",
            "Epoch... (8/20 | Eval Loss: 2.8624 | Eval rouge1: 7.0667 | Eval rouge2: 1.0 | Eval rougeL: 6.8667 | Eval rougeLsum: 6.8333 | Eval BLEU: 7.8662 | Eval gen_len: 1366.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.83it/s]\n",
            "Epoch... (9/20 | Loss: 2.584416151046753, Learning Rate: 5.516129021998495e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.69it/s]\n",
            "Epoch... (9/20 | Eval Loss: 2.8632 | Eval rouge1: 7.6667 | Eval rouge2: 1.6667 | Eval rougeL: 7.6667 | Eval rougeLsum: 7.6667 | Eval BLEU: 9.1509 | Eval gen_len: 1456.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.91it/s]\n",
            "Epoch... (10/20 | Loss: 2.927891731262207, Learning Rate: 5.016128852730617e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.83it/s]\n",
            "Epoch... (10/20 | Eval Loss: 2.8182 | Eval rouge1: 8.0 | Eval rouge2: 2.0 | Eval rougeL: 8.0 | Eval rougeLsum: 7.9333 | Eval BLEU: 9.5443 | Eval gen_len: 1379.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.87it/s]\n",
            "Epoch... (11/20 | Loss: 3.130614757537842, Learning Rate: 4.516128683462739e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.71it/s]\n",
            "Epoch... (11/20 | Eval Loss: 2.816 | Eval rouge1: 10.8 | Eval rouge2: 3.0 | Eval rougeL: 10.95 | Eval rougeLsum: 10.7167 | Eval BLEU: 8.8346 | Eval gen_len: 1307.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.87it/s]\n",
            "Epoch... (12/20 | Loss: 2.5958757400512695, Learning Rate: 4.0161292417906225e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.72it/s]\n",
            "Epoch... (12/20 | Eval Loss: 2.826 | Eval rouge1: 8.0 | Eval rouge2: 2.0 | Eval rougeL: 8.0667 | Eval rougeLsum: 8.0 | Eval BLEU: 8.453 | Eval gen_len: 1293.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.86it/s]\n",
            "Epoch... (13/20 | Loss: 2.318328380584717, Learning Rate: 3.5161290725227445e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.83it/s]\n",
            "Epoch... (13/20 | Eval Loss: 2.803 | Eval rouge1: 8.8 | Eval rouge2: 3.0 | Eval rougeL: 8.9667 | Eval rougeLsum: 8.8 | Eval BLEU: 9.4138 | Eval gen_len: 1320.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.88it/s]\n",
            "Epoch... (14/20 | Loss: 2.2813446521759033, Learning Rate: 3.016129085153807e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.70it/s]\n",
            "Epoch... (14/20 | Eval Loss: 2.7979 | Eval rouge1: 7.5 | Eval rouge2: 3.0 | Eval rougeL: 7.6 | Eval rougeLsum: 7.55 | Eval BLEU: 8.4843 | Eval gen_len: 1350.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.86it/s]\n",
            "Epoch... (15/20 | Loss: 2.556572437286377, Learning Rate: 2.516128915885929e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.71it/s]\n",
            "Epoch... (15/20 | Eval Loss: 2.8099 | Eval rouge1: 9.3333 | Eval rouge2: 3.0 | Eval rougeL: 9.4833 | Eval rougeLsum: 9.4333 | Eval BLEU: 9.9368 | Eval gen_len: 1338.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.89it/s]\n",
            "Epoch... (16/20 | Loss: 2.159782886505127, Learning Rate: 2.0161289285169914e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.67it/s]\n",
            "Epoch... (16/20 | Eval Loss: 2.7981 | Eval rouge1: 8.3333 | Eval rouge2: 3.0 | Eval rougeL: 8.55 | Eval rougeLsum: 8.3333 | Eval BLEU: 9.6109 | Eval gen_len: 1336.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.88it/s]\n",
            "Epoch... (17/20 | Loss: 2.3539769649505615, Learning Rate: 1.5161293049459346e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.74it/s]\n",
            "Epoch... (17/20 | Eval Loss: 2.7955 | Eval rouge1: 8.3333 | Eval rouge2: 3.0 | Eval rougeL: 8.55 | Eval rougeLsum: 8.3333 | Eval BLEU: 9.9381 | Eval gen_len: 1345.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.83it/s]\n",
            "Epoch... (18/20 | Loss: 2.4329521656036377, Learning Rate: 1.0161292266275268e-05)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.75it/s]\n",
            "Epoch... (18/20 | Eval Loss: 2.806 | Eval rouge1: 8.6667 | Eval rouge2: 4.0 | Eval rougeL: 8.9167 | Eval rougeLsum: 8.7167 | Eval BLEU: 9.7656 | Eval gen_len: 1338.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.87it/s]\n",
            "Epoch... (19/20 | Loss: 2.2687995433807373, Learning Rate: 5.1612910283438396e-06)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.76it/s]\n",
            "Epoch... (19/20 | Eval Loss: 2.814 | Eval rouge1: 8.6667 | Eval rouge2: 4.0 | Eval rougeL: 8.9167 | Eval rougeLsum: 8.7167 | Eval BLEU: 9.5685 | Eval gen_len: 1349.0 |)\n",
            "Training...: 100% 31/31 [00:16<00:00,  1.83it/s]\n",
            "Epoch... (20/20 | Loss: 2.337792158126831, Learning Rate: 1.612901598946337e-07)\n",
            "Evaluating...: 100% 4/4 [00:02<00:00,  1.70it/s]\n",
            "Epoch... (20/20 | Eval Loss: 2.8118 | Eval rouge1: 8.6667 | Eval rouge2: 4.0 | Eval rougeL: 8.9167 | Eval rougeLsum: 8.7167 | Eval BLEU: 9.6573 | Eval gen_len: 1357.0 |)\n",
            "Epoch... (19/20 | Eval Loss: 2.814 | Eval rouge1: 8.6667 | Eval rouge2: 4.0 | Eval rougeL: 8.9167 | Eval rougeLsum: 8.7167 | Eval BLEU: 9.5685 | Eval gen_len: 1349.0 |):  95% 19/20 [11:08<00:20, 20.17s/it]Configuration saved in /content/out/config.json\n",
            "Configuration saved in /content/out/generation_config.json\n",
            "Model weights saved in /content/out/flax_model.msgpack\n",
            "tokenizer config file saved in out/tokenizer_config.json\n",
            "Special tokens file saved in out/special_tokens_map.json\n",
            "Copy vocab file to out/spiece.model\n",
            "Epoch... (20/20 | Eval Loss: 2.8118 | Eval rouge1: 8.6667 | Eval rouge2: 4.0 | Eval rougeL: 8.9167 | Eval rougeLsum: 8.7167 | Eval BLEU: 9.6573 | Eval gen_len: 1357.0 |): 100% 20/20 [11:11<00:00, 33.57s/it]\n"
          ]
        }
      ],
      "source": [
        "!python3 t5_summary.py \\\n",
        "\t--output_dir out \\\n",
        "\t--model_name_or_path sultan/ArabicT5-Base \\\n",
        "\t--tokenizer_name sultan/ArabicT5-Base \\\n",
        "\t--train_file=\"/content/ARGEn_title_genration_sample_train.csv\" \\\n",
        "\t--validation_file=\"/content/ARGEn_title_genration_sample_valid.csv\" \\\n",
        "\t--do_train --do_eval --predict_with_generate \\\n",
        "\t--num_train_epochs 20 \\\n",
        "\t--learning_rate 1e-4 --warmup_steps 0 \\\n",
        "\t--per_device_train_batch_size 4 \\\n",
        "\t--per_device_eval_batch_size 4 \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--max_source_length 512 --max_target_length 128 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Usf_to0IhCx"
      },
      "source": [
        "Lets see now our predictions (labels on left and predictions on right). Please note that this model is trained on only ~1,000 examples from the actual dataset and thats why the BLEU score is low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_45XLTMFkn6N",
        "outputId": "b937ad4f-b4b2-45b7-9127-a845ff5b6dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label    ||    pred\n",
            "وزارة النقل والطرق: سفلتة 3 آلاف كلم بولايات الشرق الثلاث    ||    وزير النقل: تنفيذ 3 آلاف كلم من الطرق والجسور في السودان\n",
            "البنك المركزي: تلقينا طلبات لافتتاح فروع جديدة المصارف الأجنبية في السودان    ||    بنك السودان المركزي: فتح فروع للبنوك الأجنبية في السودان\n",
            "تبعا لإجراءات الترفيع في المعلوم على الاستهلاك والأداء على القيمة المضافة: الغرفة النقابية الوطنية للباعثين العقاريين تؤكد أن أسعار العقارات سترتفع    ||    الباعثين العقاريين: لا يمكن طرح الأداء على القيمة المضافة\n",
            "أسعار العملات الاجنبية مقابل الجنيه السوداني ليوم الاثنين الموافق24سبتمبر2018    ||    أسعار العملات الاجنبية مقابل الجنيه السوداني ليوم الأحد الموافق 24 سبتمبر 2018\n",
            "تخصيص 300 ألف شقة لمحدودي الدخل في الإسكان الاجتماعي وتسليم 273 ألف وحدة    ||    صندوق الإسكان الاجتماعى ودعم التمويل العقارى يخصص 500 ألف شقة لمحدودى الدخل\n",
            "الذهب يهبط لأدنى مستوى في 3 أشهر ونصف    ||    أسعار الذهب تهبط إلى أدنى مستوياتها في عامين ونصف\n",
            "حملة تونسية لمحاربة المهربين على الحدود حجز 185 مليون أورو من السلع المهربة من الجزائر إلى تونس    ||    «الديوانة» تشرع في حملة كبيرة لمكافحة التهريب\n",
            "الهيئة الملكية بالجبيل:مذكرات تفاهم مع شركات عالمية للاستثمار برأس الخير    ||    الهيئة الملكية بالجبيل توقع مذكرات تفاهم مع عدد من الشركات العالمية في مجال الصناعات التعدينية\n",
            "المجلس الوطني يوافق على تصدير إناث الثروة الحيوانية    ||    وزير الثروة الحيوانية: تصدير إناث الثروة الحيوانية خيانة عظمى للوطن\n",
            "رضا شلغوم: اقرار تمديد في الامتيازات القائمة لفائدة الشركات يعد سابقة والحكومة لا ترغب فيه حاليا    ||    وزير المالية: تمديد 3 سنوات في منظومة الامتيازات القائمة في تونس خطوة ستعزز من قدرات المؤسسات المحدثة\n",
            "صندوق النقد الدولي: الرسوم الجمركية ستضر بالاقتصاد العالمي    ||    «مجموعة العشرين» تنهي اجتماعها في بوينس آيرس\n",
            "ارتفاع أسعار العملات اليوم الأربعاء R اليورو    ||    أسعار العملات الورقية اليوم الثلاثاء 22-7-2020\n",
            "خبير تأمين: وقف تراخيص الوساطة قرار جرىء يهدف لتطوير القطاع    ||    «أبو حجر»: منع إصدار تراخيص لشركات الوساطة غير المرخص لها\n",
            "الدولار يستقر أمام الدينار عند R واليورو الى R    ||    الدولار يستقر مقابل الدينار عند مستوى 304ر0 دينار\n",
            "اجتماع لل(المالية) و(النفط) و(الكهرباء) و(المركزي) الوقود.. انفراج أزمة وتشديد عقوبات    ||    وزير المالية: لا زيادة في الوقود في السودان\n",
            "الدولار يستقر أمام الدينار عند R واليورو ينخفض إلى R    ||    الدولار يستقر مقابل الدينار عند مستوى 303ر0 دينار\n",
            "“أوطو نجمة” تسوق “مرسيدس كلاس بي” ب279 ألف درهم    ||    «أوسط نجمة» تطلق حملة تسويقية لطراز «نوكيا إكسبو 2013»\n",
            "الخطوط السعودية تعقد اجتماع مجلس الإدارة غدا    ||    وزير النقل يستكمل تشكيل مجلس إدارة المؤسسة العامة للخطوط الجوية العربية السعودية\n",
            "طلبات إعانة البطالة الأمريكية ترتفع قليلا وسوق العمل مازال متماسكا    ||    ارتفاع عدد المتقدمين بطلبات إعانة البطالة\n",
            "شركة ريم العقارية توقع اتفاقية تعاون مع شركة فيداليتي    ||    «ريم العقارية» توقع اتفاقية تعاون مع شركة فيداليتي للصيانة\n",
            "أيمن الجميل: صادرات الزراعة عبرت الأزمة ب4 ملايين طن.. ومستثمرو القطاع أبطال التنمية    ||    وزير الزراعة: قطاع الزراعة أثبت جدارة فى مواجهة الأزمة العالمية\n",
            "فواز الحكير  تتبنى حلول الحوسبة السحابية من AS لتسريع التحول الرقمي    ||    شركة فواز عبدالعزيز الحكير تنهي مرحلة التحول الرقمي إلى الحوسبة السحابية\n",
            "أشرف على افتتاح ملتقى وطني لشعبة الحليب وزير الفلاحة: يجب رفع الإنتاج لترشيد الواردات وتحقيق الاكتفاء    ||    وزير الفلاحة: رفع نسبة الإنتاج لشعبة الحليب من خلال التجميع للحليب الطازج\n",
            "مكتبة الملك عبدالعزيز تنظم ندوة ادخارك بوابة استثمارك    ||    مكتبة الملك عبدالعزيز العامة تنظم ندوة تثقيفية بعنوان ادخارك بوابة استثمارك\n",
            "بورصة دبي خضراء بجلسة بداية R المال يربح 880 مليون درهم    ||    البورصة تكسب R نقطة\n",
            "يحتاجون إلى 10 ملايين ساعة من التكوين سنويا 8 آلاف مليار لإعادة تأهيل الإطارات السامية للجزائر!    ||    راكد راكد: برنامج ماستر جزائري متخصص في مجال الصناعات التحويلية\n",
            "حسن غانم رئيسا لبنك التعمير والإسكان    ||    بنك التعمير والإسكان يبدأ رسميا ممارسة مهام منصبه الجديد كرئيس لمجلس إدارة بنك التعمير والإسكان\n",
            "احتفالية bt100 تمنح جائزة لعلى الجميل العضو المنتدب لشركة كايرو A3    ||    لجنة تحكيم bt100 تمنح جائزة إلى على الجميل العضو المنتدب لشركة كايرو 3A\n",
            "هدف يدعو المنشآت لتسليم التفويض النهائي لبرنامج دعم نمو التوطين بالمنشآت    ||    «هدف» يدعو أصحاب المنشآت إلى سرعة تسليم التفويض النهائي لبرنامج دعم نمو التوطين بالمنشآت إلى فروع هدف\n",
            "2 مليار جنيه ارتفاعا فى رأس المال السوقى للبورصة    ||    البورصة ترتفع بR مليار جنيه\n",
            "توقعات بإنتاج أكثر من 5 مليون طن من الفسفاط نهاية 2017: ارتفاع ب35 % ونقص الطلب العالمي وتراجع الأسعار يؤجل تسويق 150 ألف طن ثلاثي فسفاط الرفيع    ||    ارتفاع صادرات قطاع الفسفاط الرفيع\n",
            "مؤشر سوق الخرطوم للاوراق المالية يغلق مستقرا عند R    ||    البورصة تغلق على ارتفاع طفيف\n",
            "1.9 % نسبة التضخم فى منطقة اليورو خلال أبريل    ||    ارتفاع التضخم فى منطقة اليورو فى أبريل\n",
            "5 أسباب لتثبيت أسعار البنزين لثلاثة أشهر.. تعرف عليها    ||    لجنة التسعير التلقائى للمنتجات البترولية توصي برفع أسعار البترول\n",
            "روسيا و «أوبك» تتعاونان في دعم تثبيت أو خفض إنتاج النفط    ||    وزير الطاقة الروسي ألكسندر نوفاك: روسيا وروسيا تعززان التعاون بشأن خفض إنتاج النفط\n",
            "مشروع ب(50) مليون دولار للاستثمار في أسماك (خيار البحر)    ||    وزير الثروة الحيوانية: مشروع الاستزراع المائي في البحر الاحمر يحقق نتائج ايجابية\n",
            "تمديد إعفاء الضريبة على الدخل الإجمالي وعلى أرباح الشركات الحكومة تغري المستثمرين مجددا لإستقطاب أموال الشكارة    ||    وزارة المالية تقترح نظام جبائي تحفيزي للمستثمرين\n",
            "سعر صرف الدينار التونسي بالعملات الأجنبية لهذا اليوم    ||    سعر صرف العملة الاوروبية مقابل الجنيه التونسي ليوم الجمعة الموافق 10 سبتمبر 2019\n",
            "عينك ميزانك: تأمين الفضاء الإلكتروني    ||    «أورانج» تدشن فرعا لأمن الفضاء الإلكتروني بالمغرب\n",
            "المركزي الكويتي: وقف اصدار شهادات الاستثمار المصرية بعد تجميعها مليار جنيه    ||    «صوت الأمة»: توقف إصدار شهادات الاستثمار المصرية\n",
            "الذهب يبلغ أعلى مستوى في 6 سنوات    ||    أسعار الذهب تسجل أعلى مستوياتها في تسع سنوات\n",
            "أسعار الأسهم بالبورصة المصرية اليوم الاثنين 6-7-2020    ||    البورصة تنهي تعاملاتها بارتفاع جماعى لمؤشراتها\n",
            "البداية كانت عام 1970.. كل ما تريد معرفته عن مؤتمر اليورو مني    ||    «اليورومنى» يبدأ اجتماعاته بالقاهرة\n",
            "أرجأ الفصل في الملف إلى مارس المقبل الاتحاد الأوروبي يتماطل في شطب قائمة من السلع المعفية من الجمركة    ||    وزير التجارة: اتفاق مبدئي بين الجزائر والاتحاد الأوروبي حول التفكيك الجمركي بين الجزائر والاتحاد الأوروبي سيفتح مفاوضات جديدة حول قائمة السلع الصناعية\n",
            "100 مليون عوائد متوقعة للاتفاقيات السياحية لجهات سعودية بملتقى السفر بدبي    ||    «طيران ناس» توقع عقودا مع شركات سياحية عالمية\n",
            "مشاريع بين وزارة الدفاع ومجموعة آبار بقيمة 720 مليون دولار الشركات الإماراتية تعود إلى الجزائر بعد 10 سنوات من الإنسحاب    ||    «آبار» تعيد ثقة الشركات الإماراتية في السوق الجزائرية\n",
            "هالة السعيد : حصر الواردات ووضع خطة لتوطينها خلال المرحلة القادمة    ||    وزير التخطيط: نستهدف زيادة الواردات المصرية من R\n",
            "رئيس هيية الاستثمار يبحث خطط واستثمارات أمازون في مصر    ||    «أمازون» تلتقي بالمهندس محمد عبد الوهاب رئيس قطاع التجارة الداخلية\n",
            "المدير العام للمركز الوطني للسجل التجاري يكشف: سحب السجل التجاري عن طريق الإنترنت مع نهاية سنة 2017    ||    R مسجل كشخص معنوي\n",
            "أزمة إنعدام خبز حادة تضرب ود مدني    ||    التيار يفجر أزمة الخبز في المدينة\n",
            "السعودية للتموين تطرح 24.6 مليون سهم للاكتتاب العام    ||    \"«السعودية للتموين»: الاكتتاب العام للأفراد في 24    ||    6 مليون سهم\"\n",
            "تقييم و نجم يطلقان المرحلة الأولى من تقدير الأضرار الطفيفة إلكترونيا    ||    «تقييم» تطلق المرحلة الأولى من مبادرة تقدير الأضرار الطفيفة إلكترونيا في 30 مدينة\n",
            "الأسهم المحلية تغلق مرتفعة 11 نقطة    ||    مؤشر سوق الأسهم يغلق مرتفعا عند مستوى R نقطة\n",
            "الداخلية تطرح لوحة «س ب ع 11» للمزايدة بسعر 100 جنيه مصري    ||    «لوحتك دوت كوم»: لوحة مميزة باسم «س ع 11» بسعر 100 جنيه\n",
            "كيف تواجه شركات إعادة التأمين مخاطر الكوارث وتغير المناخ والإنترنت؟    ||    «Swiss Re» تستعرض التحديات التي تواجه صناعة التأمين\n",
            "قطاع الصناعات الغذائية.. زيادة في اجور ومنح اكثر من 120 الف عامل عبر الوطن    ||    الرابطة الوطنية لعمال الصناعات الغذائية تتفاوض مع الشركات القابضة لخوصصة مجمعات الحليب\n",
            "دعوى مقامة من علي بن عامر بن عكان الصعيري ضد عبدالعزيز بن عامر بن محمد آل خشيل    ||    لجنة الفصل في منازعات الأوراق المالية تباشر دعوى ضد «عبدالله بن عامر الصعيري»\n",
            "بمناسبة عيد الأضحى.. تعطيل العمل في البورصة من الخميس إلى الاثنين    ||    البورصة المصرية تلغي العمل بها اعتبارا من اليوم الخميس\n",
            "«برنت» يتجة لرابع خسارة أسبوعية دون 60 دولار    ||    أسعار النفط تهبط دون 50 دولارا للبرميل\n",
            "5 مغاربة في قائمة فوربس لأثرياء إفريقيا بثروة تقارب 8 مليار دولار (لائحة مفصلة)    ||    فوربس المغربية تتصدر القائمة بثروة تقدر ب6 مليارات دولار\n",
            "مستخدمو فيسبوك يتجاوزون عتبة الملياري مستخدم    ||    «فيسبوك» يتجاوز عتبة المليار مستخدم في تشرين الاول/أكتوبر\n",
            "أمير منطقة القصيم يرعى جائزة بنك الجزيرة للتميز في التربية الخاصة    ||    بنك الجزيرة يكرم الفائزين بجائزة خير الجزيرة لأهل الجزيرة للتميز في التربية الخاصة\n",
            "حملة كبرى تنتظم ولاية سنار لحصاد محصول السمسم    ||    وزير الزراعة: لن تسقط حبة سمسم واحدة\n",
            "الاستثمارات المباشرة بلغت 1 مليار دولار الأتراك يريدون غزو الجزائر تجاريا واقتصاديا    ||    R التركية تراهن على السوق الجزائرية في مجال الحديد والصلب\n",
            "إرتفاع طفيف في أسعار النفط    ||    أسعار النفط ترتفع بعد إعلان الاتفاق رسميا\n",
            "193 ألف طفل يمارسون أعمالا خطيرة بالمغرب    ||    «الديوان السامي للتخطيط» يحذر من مخاطر تشغيل الأطفال في أعمال تهدد سلامة الأطفال\n",
            "صندوق النقد الدولي يحذر من زيادة الدين العام في الدول العربية    ||    صندوق النقد يحذر من زيادة الدين العام بشكل سريع في الدول العربية المصدرة للنفط\n",
            "المهندس صالح الجاسر : مشروع البحر الأحمر نقلة عصرية في مفهوم وصناعة السياحة السعودية برؤية عالمية    ||    «السعودية» تطلق مشروعا سياحيا ضخما في المملكة\n",
            "إتحاد ولاية الخرطوم يستهدف توفير الأضاحي ل(35) ألف عامل    ||    «الصيحة»: تحديد أسعار الخراف ب(6820) جنيها\n",
            "إنفلات في الأسعار وركود بالأسواق وإضراب للصاغة    ||    «الصيحة»: توقف حركة البيع بسبب ارتفاع أسعار الدولار والدرهم\n",
            "الإحصاء: ارتفاع إنتاج الكهرباء وتراجع الاستهلاك خلال 2019.. والسولار يسجل زيادة 28 ألف طن    ||    ارتفاع كميات الكهرباء المستوردة إلى 9.5 R خلال سبتمبر\n",
            "المالية تطرح أذون خزانة بقيمة 18 مليار جنيه    ||    وزارة المالية تطرح أذون خزانة بقيمة 18 مليار جنيه\n",
            "بريطانية ترحب بمشاركة السودان في قمة الاستثمار البريطانية الأفريقية    ||    وزير الخارجية البريطاني: السودان يشارك في قمة الاستثمار البريطانية الأفريقية رفيعة المستوى\n",
            "اخبار اقتصاد اليوم.. البرلمان يوافق نهائيا على قانون المركزى والجهاز المصرفى    ||    وزير المالية: قانون البنك المركزي الجديد يغطى المتأخرات المالية\n",
            "اتحاد المصارف: دعم أهداف التنمية التزام راسخ لدفع عجلة النشاط الاقتصادي    ||    «بيتك» يؤكد دعم الاتحاد لأهداف التنمية\n",
            "رأسمالها 2 مليون دينار ومقرها الرئيسي أكودة: مجمع الزواري يعلن رسميا عن انطلاق مدرسة ESSCA الفرنسية للإدارة    ||    «الجزائر» تطلق جامعة جديدة بشهادتين\n",
            "النفط يرتفع بعد تراجع المخزونات الأمريكية    ||    أسعار النفط ترتفع بعد تقرير حكومي يظهر تحسن الطلب\n",
            "الدولار يصعد أمام العملات الرئيسية في التعاملات الاسيوية    ||    الدولار يواصل ارتفاعه مقابل العملات الرئيسية\n",
            "“المالية”: تجاوز المجموع الكلي لطلبات الاكتتاب في الصكوك المحلية ١ مليار ريال    ||    وزارة المالية تعلن عن إقبال كبير على إصدار صكوك المملكة بالريال\n",
            "تراجع حاد جديد لمخزونات النفط الخام الأمريكية    ||    ارتفاع مخزونات الخام بالولايات المتحدة رغم قيام مصافي التكرير بخفض الإنتاج\n",
            "البورصة المصرية تتراجع مع تعيين رئيس الوزراء الجديد    ||    البورصة تخسر R بسبب تعيين قنديل وزيرا الري\n",
            "سعر سلة خامات أوبك يتراجع إلى R ولار للبرميل    ||    أوبك تعلن عن خفض انتاجها\n",
            "شركة (ويتا) تباشر طحن القمح وتبدأ التوزيع غدا    ||    «ويتا» للغلال يعلن بدء توزيع (48) ألف طن قمح محلي\n",
            "الجزائر تستعين بحبوب لكينوا للرفع من المردود    ||    الكينوا تشرع في تجارب لإدخال حبوب الكينوا\n",
            "كوريا الشمالية تطلق صاروخا باليستيا «ثانيا» عابرا للقارات خلال 24 ساعة    ||    كوريا الشمالية تطلق صاروخا باليستيا عابرا للقارات\n",
            "المسكن الأول: محور الدورة السابعة للمعرض الدولي للبنوك والخدمات المالية    ||    الباعثين الخواص يستقطب مشاركة واسعة من الباعثين الخواص\n",
            "أسعار العملات اليوم الأثنين 26-11-2018 فى مصر    ||    أسعار العملات اليوم الأحد 26-11-2018\n",
            "انخفاض في اسعار الذهب في معاملات اليوم الاربعاء    ||    سعر الجرام في التداول العالمي اليوم الأربعاء\n",
            "شكاوى من ارتفاع أسعار الأدوية منها المصنعة داخل السودان    ||    «الجريدة» تطالب وزارة الصحة بالتدخل العاجل لحل أزمة الأدوية العلاجية\n",
            "البورصة تقفز بنسبة R بسبب مشتريات المصريين    ||    البورصة تحقق طفرة كبيرة خلال تعاملات اليوم الأحد.. ومؤشرات الأجانب تسجل طفرة كبيرة\n",
            "مسؤول بهيرميس: الدولة تهتم بتنفيذ برنامج الطروحات الحكومية    ||    «هيرميس» تطرح 4 شركات بالبورصة خلال النصف الثانى من العام الجارى\n",
            "أسعارالعملات الاجنبية مقابل الجنيه السوداني اليوم السبت الموافق 10/09/2016    ||    أسعار العملات الاجنبية مقابل الجنيه السوداني ليوم السبت الموافق 10 سبتمبر 2016\n",
            "انخفاض احتياطي الجزائر الأجنبي 6.1 مليار دولار منذ بداية العام    ||    البنك المركزي: احتياطي الجزائر من النقد الأجنبي انخفض 5.8 مليار دولار\n",
            "تباين مؤشرات R والسيولة تتراجع إلى 27.6 مليون دينار    ||    البورصة تخسر نقطة مئوية\n",
            "تعاملات الأجانب تعاني ضعف الشفافية وتؤرق المستثمرين    ||    «الجريدة» تؤكد تبعية تعاملات الأجانب للسوق\n",
            "نائب رئيس «الأهلي»: 35 بنكا ضمن قائمة أكبر 100 ممول ضريبي في مصر    ||    البنك الاهلي: مصر تملك قطاعا مصرفيا قويا\n",
            "أسهم الشركة التونسية للبنك تواصل صعودها    ||    البورصة التونسية تفتتح تعاملاتها على ارتفاع\n",
            "تذبذب مؤشرات البورصة في افتتاح جلسة اليوم    ||    البورصة تبدأ تعاملاتها على ارتفاع المؤشرات\n",
            "الحموشي يلغي تمبر 20 درهم في محاضر التبليغ عن السرقة وحوادث السير    ||    الشرطة تلغي طوابع مغربية من فئة 20 درهما\n",
            "بها فرص استثمارية فى مختلف المجالات.. ماذا قال السفير الألماني عن الاقتصاد المصري؟    ||    وزير الخارجية الألماني يؤكد على الفرص الاستثمارية الواعدة للاقتصاد المصري\n"
          ]
        }
      ],
      "source": [
        "!cat /content/pred.csv | sed  's/,/    ||    /g'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgv1CwNWYNvO"
      },
      "source": [
        "# **Running Flax Code on Text Classificaiton (Sentiment Analysis)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ6uxa8Df5yZ"
      },
      "source": [
        "In this example, we will finetune ArabicT5 on ArSarcasm-v2, which is part of WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. ArSarcasm-v2 has two tasks :\n",
        " * Sentimental Analysis (POS, NEG, NEU)\n",
        " * Scarcasm (True, False)\n",
        "\n",
        "Here we will focus on the Sentimental Analysis task. The official evaluation metric for the Sentimental Analysis task is F1 PN which only counts POS and NEG classes.\n",
        "\n",
        "Refer to this paper to get reported results of other Arabic NLP models:\n",
        "\n",
        "[Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection](https://aclanthology.org/2021.wanlp-1.3/).  Please note that seq2seq models like GPT-2, BART, and T5 are mainly designed to target generative tasks like Machine Translation, summarization, and Question Generation. However, here we extend the evaluation of T5 models to Text Classification tasks to show another case of T5 applications. Abstractive (extractive) language models like AraBERT, AraELECTRA, and our model [ArabicTransformer](https://aclanthology.org/2021.findings-emnlp.108/) should perform better in these tasks.\n",
        "\n",
        "\n",
        "Here we did a small change to the Question Answering FLAX code in line 665 by having this code :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for sentence,label in zip(examples[\"sentence\"],examples[\"label\"]):\n",
        "            inputs.append(\"<s> %s\" % (sentence))\n",
        "            targets.append(\"<s> %s\" % (label))\n",
        "```\n",
        "and we also added a function to calculate F1 PN score for in line 603 :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def calc_scarcasm_score(y_pred,y_true):\n",
        "        accuracy=accuracy_score(y_true, y_pred)\n",
        "        f1_pn=f1_score(y_true, y_pred,labels=['negative','positive'],average=\"macro\")\n",
        "        return accuracy,f1_pn\n",
        "```\n",
        "\n",
        "\n",
        "and use this function in line 721\n",
        "\n",
        "```\n",
        "result={}\n",
        "accuracy,f1_pn=calc_scarcasm_score(decoded_preds,decoded_labels)\n",
        "result[\"Accuracy\"]=accuracy\n",
        "result[\"F1_PN\"]=f1_pn\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2CFl8IMYNvS",
        "outputId": "9aa2330d-8d34-481a-ac94-113f906ff3ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting t5_text_class.py\n"
          ]
        }
      ],
      "source": [
        "#@title T5 FLAX Text Classification\n",
        "%%writefile t5_text_class.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "    def calc_scarcasm_score(y_pred,y_true):\n",
        "        accuracy=accuracy_score(y_true, y_pred)\n",
        "        f1_pn=f1_score(y_true, y_pred,labels=['negative','positive'],average=\"macro\")\n",
        "        return accuracy,f1_pn\n",
        "\n",
        "\n",
        "        \n",
        "    def preprocess_function(examples):\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "\n",
        "        for tweet,sentiment in zip(examples[\"tweet\"],examples[\"sentiment\"]):\n",
        "            inputs.append(\"%s\" % (tweet))\n",
        "            targets.append(\"%s\" % (sentiment))\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    \n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [norm_seq(pred.strip()) for pred in preds]\n",
        "        labels = [norm_seq(label.strip()) for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result={}\n",
        "        accuracy,f1_pn=calc_scarcasm_score(decoded_preds,decoded_labels)\n",
        "        result[\"Accuracy\"]=accuracy\n",
        "        result[\"F1_PN\"]=f1_pn\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG0VrUmTYNvV",
        "outputId": "2316ef14-5505-4fa0-d33c-0e3d0429da98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 20:00:34--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585081 (571K) [text/plain]\n",
            "Saving to: ‘arsarcasm-v2_dev.csv’\n",
            "\n",
            "arsarcasm-v2_dev.cs 100%[===================>] 571.37K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-04-05 20:00:34 (12.7 MB/s) - ‘arsarcasm-v2_dev.csv’ saved [585081/585081]\n",
            "\n",
            "--2023-04-05 20:00:34--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393168 (2.3M) [text/plain]\n",
            "Saving to: ‘arsarcasm-v2_train.csv’\n",
            "\n",
            "arsarcasm-v2_train. 100%[===================>]   2.28M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-04-05 20:00:35 (34.9 MB/s) - ‘arsarcasm-v2_train.csv’ saved [2393168/2393168]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O arsarcasm-v2_dev.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
        "!wget -O arsarcasm-v2_train.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
        "''' \n",
        "we find an issue with T5 model when label are [\"POS\",\"NEG\",\"NEU\"] and model would truncate last letter so \n",
        "we will replace them to full words. This should not affect the evaluation or training on this task\n",
        "'''\n",
        "### we will \n",
        "!sed -i 's/POS/positive/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/NEG/negative/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/NEU/neutral/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/POS/positive/g' arsarcasm-v2_train.csv\n",
        "!sed -i 's/NEG/negative/g' arsarcasm-v2_train.csv\n",
        "!sed -i 's/NEU/neutral/g' arsarcasm-v2_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWqoSxjIYNvV",
        "outputId": "065b7b0e-096a-4a0a-9bb2-082f99825f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 20:18:19.738729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, per_device_train_batch_size=3, per_device_eval_batch_size=4, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=10.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-548287080ddd0c9c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 8160.12it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1300.76it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-548287080ddd0c9c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 759.77it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/13109f647d4288928d6e56bc6b76d0d1db729814/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('encoder', 'embed_tokens', 'kernel'), ('decoder', 'embed_tokens', 'kernel'), ('lm_head', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 12548\n",
            "INFO:__main__:  Num Epochs = 10\n",
            "INFO:__main__:  Instantaneous batch size per device = 3\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 24\n",
            "INFO:__main__:  Total optimization steps = 5220\n",
            "Training...: 100% 522/522 [06:43<00:00,  1.30it/s]\n",
            "Epoch... (1/10 | Loss: 0.10088685899972916, Learning Rate: 9.00191516848281e-05)\n",
            "Evaluating...:   0% 0/94 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 94/94 [01:32<00:00,  1.01it/s]\n",
            "Epoch... (1/10 | Eval Loss: 0.198 | Eval Accuracy: 65.8 | Eval F1_PN: 62.2441 |)\n",
            "Training...: 100% 522/522 [03:37<00:00,  2.40it/s]\n",
            "Epoch... (2/10 | Loss: 0.15755793452262878, Learning Rate: 8.001915557542816e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 14.55it/s]\n",
            "Epoch... (2/10 | Eval Loss: 0.2048 | Eval Accuracy: 67.9667 | Eval F1_PN: 68.6027 |)\n",
            "Training...: 100% 522/522 [03:37<00:00,  2.40it/s]\n",
            "Epoch... (3/10 | Loss: 0.20019082725048065, Learning Rate: 7.00191521900706e-05)\n",
            "Evaluating...: 100% 94/94 [00:05<00:00, 15.97it/s]\n",
            "Epoch... (3/10 | Eval Loss: 0.1914 | Eval Accuracy: 67.7333 | Eval F1_PN: 68.9027 |)\n",
            "Training...: 100% 522/522 [03:38<00:00,  2.39it/s]\n",
            "Epoch... (4/10 | Loss: 0.1248922124505043, Learning Rate: 6.0019156080670655e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 14.34it/s]\n",
            "Epoch... (4/10 | Eval Loss: 0.1936 | Eval Accuracy: 68.6667 | Eval F1_PN: 70.2585 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.38it/s]\n",
            "Epoch... (5/10 | Loss: 0.12645971775054932, Learning Rate: 5.00191563332919e-05)\n",
            "Evaluating...: 100% 94/94 [00:05<00:00, 15.80it/s]\n",
            "Epoch... (5/10 | Eval Loss: 0.1939 | Eval Accuracy: 69.1667 | Eval F1_PN: 70.9457 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.38it/s]\n",
            "Epoch... (6/10 | Loss: 0.08181535452604294, Learning Rate: 4.0019152947934344e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 15.54it/s]\n",
            "Epoch... (6/10 | Eval Loss: 0.2027 | Eval Accuracy: 67.9667 | Eval F1_PN: 69.9681 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.38it/s]\n",
            "Epoch... (7/10 | Loss: 0.09298112988471985, Learning Rate: 3.00191568385344e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 14.91it/s]\n",
            "Epoch... (7/10 | Eval Loss: 0.2128 | Eval Accuracy: 67.9333 | Eval F1_PN: 69.9943 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.38it/s]\n",
            "Epoch... (8/10 | Loss: 0.0963783711194992, Learning Rate: 2.0019155272166245e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 14.84it/s]\n",
            "Epoch... (8/10 | Eval Loss: 0.209 | Eval Accuracy: 68.3333 | Eval F1_PN: 70.4047 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.37it/s]\n",
            "Epoch... (9/10 | Loss: 0.11608053743839264, Learning Rate: 1.0019152796303388e-05)\n",
            "Evaluating...: 100% 94/94 [00:05<00:00, 16.22it/s]\n",
            "Epoch... (9/10 | Eval Loss: 0.2111 | Eval Accuracy: 68.1 | Eval F1_PN: 70.3054 |)\n",
            "Training...: 100% 522/522 [03:39<00:00,  2.37it/s]\n",
            "Epoch... (10/10 | Loss: 0.1111082062125206, Learning Rate: 1.9156932040687025e-08)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 15.14it/s]\n",
            "Epoch... (10/10 | Eval Loss: 0.2109 | Eval Accuracy: 68.3667 | Eval F1_PN: 70.3698 |)\n",
            "Epoch... (9/10 | Eval Loss: 0.2111 | Eval Accuracy: 68.1 | Eval F1_PN: 70.3054 |):  90% 9/10 [42:33<03:53, 233.51s/it]Configuration saved in /content/out/config.json\n",
            "Configuration saved in /content/out/generation_config.json\n",
            "Model weights saved in /content/out/flax_model.msgpack\n",
            "tokenizer config file saved in out/tokenizer_config.json\n",
            "Special tokens file saved in out/special_tokens_map.json\n",
            "Copy vocab file to out/spiece.model\n",
            "Epoch... (10/10 | Eval Loss: 0.2109 | Eval Accuracy: 68.3667 | Eval F1_PN: 70.3698 |): 100% 10/10 [42:37<00:00, 255.78s/it]\n"
          ]
        }
      ],
      "source": [
        "!python3 t5_text_class.py \\\n",
        "\t--output_dir out \\\n",
        "\t--model_name_or_path sultan/ArabicT5-Base \\\n",
        "\t--tokenizer_name sultan/ArabicT5-Base \\\n",
        "\t--train_file=\"/content/arsarcasm-v2_train.csv\" \\\n",
        "\t--validation_file=\"/content/arsarcasm-v2_dev.csv\" \\\n",
        "\t--do_train --do_eval --predict_with_generate \\\n",
        "\t--num_train_epochs 10 \\\n",
        "\t--learning_rate 1e-4 --warmup_steps 0 \\\n",
        "\t--per_device_train_batch_size 3 \\\n",
        "\t--per_device_eval_batch_size 4 \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--max_source_length 128 --max_target_length 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that our best F1_PN score is 70.4 and this exceeded other seq2seq models like AraGPT2  [Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection.](https://aclanthology.org/2021.wanlp-1.3/) <- Table 2 - Sentiment Analysis section"
      ],
      "metadata": {
        "id": "9VMqAH4NtqKE"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}