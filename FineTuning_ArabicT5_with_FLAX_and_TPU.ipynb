{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J0yGl14VQIj"
      },
      "source": [
        "# **Env. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEOC_PiVVjx"
      },
      "source": [
        "pip installation: Colab TPU\n",
        "Colab TPU runtimes come with JAX pre-installed, but before importing JAX you must run the following code to initialize the TPU (we added these lines to our finetuning code already). This code should be part of your python file, not in the colab cell :\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "Colab TPU runtimes use an older TPU architecture than Cloud TPU VMs, so installing jax[tpu] should be avoided on Colab. If, for any reason, you would like to update the jax & jaxlib libraries on a Colab TPU runtime, follow the CPU instructions above (i.e., install jax[cpu]).\n",
        "\n",
        "https://github.com/google/jax#pip-installation-colab-tpu\n",
        "\n",
        "FLAX (JAX) is also flexible to work with GPU, and to make it work with GPU; it is better to remove the below lines from each code. You will find these lines at the beginning of each finetuning code in our examples.\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbCTAfpnYLCr",
        "outputId": "36741da9-7f78-466e-f9e7-a3c666a25dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.1)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (1.25.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers==4.26.1\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip3 install evaluate\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install fuzzysearch\n",
        "!pip3 install bert_score\n",
        "!pip3 install rouge_score\n",
        "!pip3 install camel_tools\n",
        "!pip3 install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67YDTYYjc5V"
      },
      "source": [
        "Jaxlib and Jax of Google Colab use a special version of Jaxlib that work with google colab. A newer version of jaxlib may only work with TPU VM devices. So first, we need to check what Jax and Jaxlib versions this Google colab uses, and then we will force the installation of Flax to match both the current Jax and Jaxlib versions.\n",
        "\n",
        "We also added wandb to our codes so you can log your experiments. More about wandb https://wandb.ai .\n",
        "\n",
        "If you want to disable wandb, run the cell below :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edmkUfBAENC",
        "outputId": "f1154f7e-681d-4529-e102-f5fbb12b35ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n",
            "W&B disabled.\n"
          ]
        }
      ],
      "source": [
        "!wandb offline\n",
        "!wandb disabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErmTwuXejY3C",
        "outputId": "70450e8d-a9da-4716-d348-a092d68f15a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.25\n",
            "0.3.25\n"
          ]
        }
      ],
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "import jax\n",
        "import jaxlib\n",
        "print(jax.__version__)\n",
        "print(jaxlib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8-gXGUghmvo",
        "outputId": "c325b5a1-a334-4a78-ee3b-f96bffaca2c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax==0.3.25 in /usr/local/lib/python3.10/dist-packages (0.3.25)\n",
            "Requirement already satisfied: jaxlib==0.3.25 in /usr/local/lib/python3.10/dist-packages (0.3.25)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.6.9)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from jax==0.3.25) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.3.25) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax==0.3.25) (1.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from jax==0.3.25) (4.6.3)\n",
            "INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.10-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.8-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.0/216.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.7-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.2/214.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.6-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading flax-0.6.4-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flax) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.5)\n",
            "Collecting orbax (from flax)\n",
            "  Downloading orbax-0.1.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.36)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.7)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (1.5.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (5.12.0)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (1.2.0)\n",
            "INFO: pip is looking at multiple versions of orbax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading orbax-0.1.6-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.5-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.4-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading orbax-0.1.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
            "INFO: pip is looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting chex>=0.1.5 (from optax->flax)\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->flax) (1.16.0)\n",
            "Installing collected packages: chex, orbax, flax\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.7\n",
            "    Uninstalling chex-0.1.7:\n",
            "      Successfully uninstalled chex-0.1.7\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.6.9\n",
            "    Uninstalling flax-0.6.9:\n",
            "      Successfully uninstalled flax-0.6.9\n",
            "Successfully installed chex-0.1.6 flax-0.6.4 orbax-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install jax==0.3.25 jaxlib==0.3.25 flax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQ02AciJkGF"
      },
      "source": [
        "# **General Direction in how to use FLAX and T5 Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwsJKblDnV0-"
      },
      "source": [
        "Seq2Seq model work simply by having an input text and target text, and you add tags for each one. In contrast to extractive models like BERT, ELECTRA, and ALBERT, we use the same input-target format for all tasks. In this colab, we will adapt this code https://github.com/huggingface/transformers/blob/main/examples/flax/summarization/run_summarization_flax.py . If you go down, you will find code called \"T5 QA\" which is an adapation of summarization code for QA tasks.\n",
        "\n",
        "For example:\n",
        "- let's say that you try to target machine translation and you have a CSV file that has the first column \"english_text\" and a second column \"arabic_text\", then your code will look like this  :\n",
        "```\n",
        "for english_text,arabic_text in zip(examples[\"english_text\"],examples[\"arabic_text\"]):\n",
        "            inputs.append(\"english : %s\" % (english_text))\n",
        "            targets.append(\"arabic : %s\" % (arabic_text))\n",
        "```\n",
        "After the code reads all examples, your input and output will look like this :\n",
        "```\n",
        "input: <english : artificial intelligence is the future>\n",
        "target: <arabic : الذكاء الاصطناعي هو المستقبل >\n",
        "```\n",
        "We use the zip function to combine two entries, and we use %s to insert variables that come after % mark to our text. If you have two entries (e.g., in question answering task, we have both context and question as input), then you can have something like this :\n",
        "```\n",
        "for context,question,answer in zip(examples[\"context\"],examples[\"question\"],examples['answers']):\n",
        "            inputs.append(\"question: %s context: %s\" % (question,context))\n",
        "            targets.append(\"answer: %s\" % (answer[\"text\"]))\n",
        "```\n",
        "Your example after preprocessing will look like this :\n",
        "```\n",
        " input: <question: ماهي عاصمة ايطاليا context : روما هي عاصمة ايطاليا واكبر مدنها >\n",
        " target: < روما >\n",
        "```\n",
        "- For Sentiment Analysis :\n",
        "Assuming you have a column named \"sentence\" and \"labels\" in your CSV files, then you can have a code like this.\n",
        "```\n",
        "for sentence,label in zip(examples[\"sentence\"],examples[\"label\"]):\n",
        "            inputs.append(\"%s\" % (sentence))\n",
        "            targets.append(\"%s\" % (label))\n",
        "```\n",
        "Your example after preprocessing will look like this:\n",
        "```\n",
        " input: <التلفاز الذي قمت بشراءه من المتجر لم يعجبني >\n",
        " target: < negative >\n",
        "```\n",
        "\n",
        "- For Text Summarization :\n",
        "Assuming you have a column named \"article\" and \"summary\" in your CSV files, then you can have a code like this\n",
        "```\n",
        "for article,summary in zip(examples[\"article\"],examples[\"summary\"]):\n",
        "            inputs.append(\"%s\" % (article))\n",
        "            targets.append(\"%s\" % (summary))\n",
        "```\n",
        "\n",
        "\n",
        "Observe also that we did not use prefixes in examples where we have one entry, but we need it when our example consists of two entries, like question answering.\n",
        "\n",
        "\n",
        "You also need to prefix even if your example has one entry if you are planning to finetune the T5 model to target different tasks. For example, if you fine-tune your model in question answering first. You add a prefix in the input and target. Then you finetune it in sentiment analysis by adding a prefix also. The model then will understand which task you are targeting during the inference by detecting those prefixes.\n",
        "\n",
        "Suppose you want to use a different evaluation metric. In that case, you can add your evaluation metric method (e.g., F1, Accuracy .. ) in lines 765-775 under  compute_metrics function.\n",
        "\n",
        "Huggingface evaluate library has a list of evaluation metrics that you can load using evaluate.load(\"\") function.\n",
        "\n",
        "For example, to add a function for EM/F1 score for Arabic TyDi Evaluation, we first add these lines to line 605 (this EM/F1 function is adapted from this [colab](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def normalize_answer(s):\n",
        "\n",
        "      def remove_articles(text):\n",
        "          return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "      def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "\n",
        "      def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "      def lower(text):\n",
        "          return text.lower()\n",
        "\n",
        "      def ar_normalize(text):\n",
        "          text = text.strip()\n",
        "          text = normalize_alef_ar(text)\n",
        "          text = normalize_alef_maksura_ar(text)\n",
        "          text = normalize_teh_marbuta_ar(text)\n",
        "          text=  araby.strip_diacritics(text)\n",
        "          return text\n",
        "\n",
        "      return ar_normalize(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
        "\n",
        "\n",
        "    def f1_score(prediction, ground_truth):\n",
        "        prediction_tokens = normalize_answer(prediction).split()\n",
        "        ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "\n",
        "    def exact_match_score(prediction, ground_truth):\n",
        "        return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "    def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "        scores_for_ground_truths = []\n",
        "        for ground_truth in ground_truths:\n",
        "            score = metric_fn(prediction, ground_truth)\n",
        "            scores_for_ground_truths.append(score)\n",
        "        return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "    def evaluate_qa(gold_answers, predictions):\n",
        "        f1 = exact_match = total = 0\n",
        "\n",
        "        for ground_truths, prediction in zip(gold_answers, predictions):\n",
        "          total += 1\n",
        "          exact_match += metric_max_over_ground_truths(\n",
        "                        exact_match_score, prediction, ground_truths)\n",
        "          f1 += metric_max_over_ground_truths(\n",
        "              f1_score, prediction, ground_truths)\n",
        "\n",
        "        exact_match = exact_match / total\n",
        "        f1 = f1 / total\n",
        "        return exact_match,f1\n",
        "```\n",
        "\n",
        "Then, we use this evaluation function by reporting its results in lines 709 :\n",
        "\n",
        "\n",
        "```\n",
        " result[\"EM\"],result[\"F1\"]=evaluate_qa(references,decoded_preds)\n",
        " ```\n",
        "\n",
        "Basically, you need to insert any evaluation score you want the code to show it up during the evaluation to [result] dictionary inside compute_metrics function.\n",
        "\n",
        "Also, observe that in line 69, we add this code :\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "You can remove it if you are using GPU.\n",
        "\n",
        "Finally, if you prefer to work with torch code instead of FLAX, you can make the same change we did it here to https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py . Both FLAX and Torch codes are very similar except for small changes to fit the FLAX environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ops9-aMOVK5m"
      },
      "source": [
        "# **Running Flax Code on Question Answering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIu8ZvqJ0_7a"
      },
      "source": [
        "First, let's process the Arabic TyDi QA dataset. TyDi combines all questions for more than 11 different languages, so we need to extract the Arabic portion and also do some pre-processing. All credit to AUB Mind LAB mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oefGTYcdoZpZ",
        "outputId": "f415f87e-7766-4022-b784-0dffbbe7112f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 30.20 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aub-mind/arabert\n",
        "!cp arabert/examples/question-answering/utils_qa.py .\n",
        "!cp arabert/examples/question-answering/trainer_qa.py .\n",
        "!cp arabert/examples/question-answering/run_qa.py .\n",
        "!cp arabert/examples/question-answering/squad_preprocessing.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrRzVWNocBh",
        "outputId": "ad40b92a-0894-44fd-a1fc-2e55a3d8d062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-13 19:20:23--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.103.128, 108.177.120.128, 142.250.159.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.103.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58004076 (55M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-train.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-t 100%[===================>]  55.32M   180MB/s    in 0.3s    \n",
            "\n",
            "2023-06-13 19:20:23 (180 MB/s) - ‘tydiqa-goldp-v1.1-train.json’ saved [58004076/58004076]\n",
            "\n",
            "--2023-06-13 19:20:23--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.103.128, 108.177.120.128, 142.250.159.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.103.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5617409 (5.4M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-dev.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-d 100%[===================>]   5.36M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-06-13 19:20:23 (145 MB/s) - ‘tydiqa-goldp-v1.1-dev.json’ saved [5617409/5617409]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGIdA3eJoeRZ"
      },
      "outputs": [],
      "source": [
        "model_name=\"sultan/ArabicT5-Large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68MU-v3ogL7",
        "outputId": "c8aa7a21-160a-4900-d29a-f2dd52f3da90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-13 19:20:26.852764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "sultan/ArabicT5-Large\n",
            "W0613 19:20:28.659094 140288135685952 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            " 38% 18948/49881 [00:00<00:00, 187073.91it/s]WARNING:tensorflow:Could not find answer for question 'arabic-1804271180688859213-0' :\n",
            " 'الإسكندر الثالث المقدوني ، المعروف بأسماء عديدة أخرى أبرزها : الإسكندر الأكبر ، و < b data - parsoid = ' { \" dsr \" : [1849 , 1870 , 3 , 3] } ' > الإسكندر الكبير ، و < b data - parsoid = ' { \" dsr \" : [1873 , 1896 , 3 , 3] } ' > الإسكندر المقدوني ، و < b data - parsoid = ' { \" dsr \" : [1899 , 1924 , 3 , 3] } ' > الإسكندر ذو القرنين ( باليونانية : ؛ نقحرة : ) ، هو أحد ملوك مقدونيا الإغريق ، ومن أشهر القادة العسكريين والفاتحين عبر التاريخ . ولد الإسكندر في مدينة يلا قرابة سنة 356 ق . م ، وتتلمذ على يد الفيلسوف والعالم الشهير أرسطو حتى بلغ ربيعه السادس عشر . وبحلول عامه الثلاثين ، كان قد أسس إحدى أكبر وأعظم الإمبراطوريات التي عرفها العالم القديم ، والتي امتدت من سواحل البحر الأيوني غربا وصولا إلى سلسلة جبال الهيمالايا شرقا . يعد أحد أنجح القادة العسكريين في مسيرتهم ، إذ لم يحصل أن هزم في أي معركة خاضها على الإطلاق . [1]' \n",
            "vs.\n",
            " 'Ἀλέξανδρο'\n",
            "orig answer:\n",
            " 'Ἀλέξανδρο'\n",
            "==================\n",
            "100% 49881/49881 [00:17<00:00, 2899.30it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 1 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-train.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-train-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhP2gAHohvr",
        "outputId": "054de447-076c-4103-d8ca-494d70eabe46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-13 19:20:54.100123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "sultan/ArabicT5-Large\n",
            "W0613 19:20:55.897143 140151005525824 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            "100% 5077/5077 [00:01<00:00, 4842.00it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 0 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-dev.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-dev-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsNd92609mxl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "def squad_json_to_csv(json_file,csv_output):\n",
        " with open(csv_output,  'w', newline='') as csvfile:\n",
        "    tsv_writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
        "    with open(json_file, 'r') as tydi_json:\n",
        "      data = json.loads(tydi_json.read())\n",
        "      tydi=data[\"data\"]\n",
        "      tsv_writer.writerow([\"id\",\"context\",\"title\",\"question\",\"answer1\",\"answer2\"])\n",
        "      for entry in tydi:\n",
        "        if \"arabic\" in entry[\"paragraphs\"][0][\"qas\"][0][\"id\"]:\n",
        "            answers=[] ### in some question of TyDi there is more than one answer (ground truths) for the question.\n",
        "            for answer in entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"]:\n",
        "             answers.append(answer[\"text\"])\n",
        "            if len(answers)==1: ### if there is one answer then add the first one to answer1 column and empty \"\" value to answer2\n",
        "             tsv_writer.writerow([entry[\"paragraphs\"][0][\"qas\"][0][\"id\"],entry[\"paragraphs\"][0][\"context\"],entry[\"title\"],entry[\"paragraphs\"][0][\"qas\"][0][\"question\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"],\"-\"])\n",
        "            else: ## if there are two answers, add the first one to answer1 column and second one to answer2 column\n",
        "             tsv_writer.writerow([entry[\"paragraphs\"][0][\"qas\"][0][\"id\"],entry[\"paragraphs\"][0][\"context\"],entry[\"title\"],entry[\"paragraphs\"][0][\"qas\"][0][\"question\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"],entry[\"paragraphs\"][0][\"qas\"][0][\"answers\"][1][\"text\"]])\n",
        "squad_json_to_csv(\"tydiqa-goldp-v1.1-train-pre.json\",\"tydiqa-goldp-v1.1-train-pre.csv\")\n",
        "squad_json_to_csv(\"tydiqa-goldp-v1.1-dev-pre.json\",\"tydiqa-goldp-v1.1-dev-pre.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiXpjSg9PEAx"
      },
      "source": [
        "Below we used the code \"https://github.com/huggingface/transformers/tree/main/examples/flax/summarization\" at the huggingface library, and we modified it for our task. You should click on the play button beside \"show code\" so the code will be written to this colab. If you want to use this code on your local machine, which has a GPU, you can simply save this written file to your local machine and run the code.\n",
        "\n",
        "Make sure to remove the line\n",
        "\n",
        "```\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "if you are planning to use your GPU or if you want to use TPU VM directly on Google Cloud Compute.\n",
        "\n",
        "\n",
        "In addtion, to make our code work with our baseline model AraBART we change the following code :\n",
        "\n",
        "```\n",
        "decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "```\n",
        "\n",
        "with :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "if \"MBartModel\" in str(config.architectures):\n",
        "    decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id)\n",
        "else:\n",
        "    decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "```\n",
        "\n",
        "This is because AraBART uses MBart architecture which take only two arguments for its shift right tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzDOj2pcQ6Q",
        "outputId": "e35c8f14-2296-4515-8aa2-4c324b0e2f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing t5_qa.py\n"
          ]
        }
      ],
      "source": [
        "#@title T5 QA\n",
        "%%writefile t5_qa.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "import pyarabic.araby as araby\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "    def normalize_answer(s):\n",
        "\n",
        "      def remove_articles(text):\n",
        "          return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "      def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "\n",
        "      def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "      def lower(text):\n",
        "          return text.lower()\n",
        "\n",
        "      def ar_normalize(text):\n",
        "          text = text.strip()\n",
        "          text = normalize_alef_ar(text)\n",
        "          text = normalize_alef_maksura_ar(text)\n",
        "          text = normalize_teh_marbuta_ar(text)\n",
        "          text=  araby.strip_diacritics(text)\n",
        "          return text\n",
        "\n",
        "      return ar_normalize(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
        "\n",
        "\n",
        "    def f1_score(prediction, ground_truth):\n",
        "        prediction_tokens = normalize_answer(prediction).split()\n",
        "        ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "\n",
        "    def exact_match_score(prediction, ground_truth):\n",
        "        return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "    def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "        scores_for_ground_truths = []\n",
        "        for ground_truth in ground_truths:\n",
        "            score = metric_fn(prediction, ground_truth)\n",
        "            scores_for_ground_truths.append(score)\n",
        "        return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "    def evaluate_qa(gold_answers, predictions):\n",
        "        f1 = exact_match = total = 0\n",
        "\n",
        "        for ground_truths, prediction in zip(gold_answers, predictions):\n",
        "          total += 1\n",
        "          exact_match += metric_max_over_ground_truths(\n",
        "                        exact_match_score, prediction, ground_truths)\n",
        "          f1 += metric_max_over_ground_truths(\n",
        "              f1_score, prediction, ground_truths)\n",
        "\n",
        "        exact_match = exact_match / total\n",
        "        f1 = f1 / total\n",
        "        return exact_match,f1\n",
        "\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        #inputs = examples[\"context_question\"]\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "        #### Question Answering ####\n",
        "        ## although for some question in TyDi we have more than one answer, we will only use first one for training. For evaluation we will use both.\n",
        "        for context,question,answer in zip(examples[\"context\"],examples[\"question\"],examples['answer1']):\n",
        "            inputs.append(\"question: %s context: %s\" % (question,context))\n",
        "            targets.append(\"%s\" % (answer))\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        if \"MBartModel\" in str(config.architectures):\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id)\n",
        "        else:\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [norm_seq(pred.strip()) for pred in preds]\n",
        "        labels = [norm_seq(label.strip()) for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        ### add below lines if you want to calcuate rouge results #####\n",
        "        #metric = evaluate.load(\"rouge\")\n",
        "        #result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "        ##############################\n",
        "        #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        #result[\"BLEU\"]=evaluate.load(\"bleu\").compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
        "        #result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        #### this code block for TyDi evaluation. Remove it if you have different task ###\n",
        "        result={}\n",
        "        tydi_dev_file=pd.read_csv(data_args.validation_file)\n",
        "        references=[]\n",
        "        for index,row in tydi_dev_file.iterrows():\n",
        "          answers=[]\n",
        "          answers.append(str(row[\"answer1\"]))\n",
        "          if str(row[\"answer2\"])!=\"\":\n",
        "            answers.append(str(row[\"answer2\"]))\n",
        "          references.append(answers)\n",
        "        result[\"EM\"],result[\"F1\"]=evaluate_qa(references,decoded_preds)\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaVWsUF8IP15",
        "outputId": "538243b0-0efd-43eb-dd8b-5a541247ff94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-13 19:22:48.449100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=9.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c3e0da5085c6c983/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 6949.97it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1072.44it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c3e0da5085c6c983/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 688.44it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('decoder', 'embed_tokens', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('lm_head', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 14805\n",
            "INFO:__main__:  Num Epochs = 9\n",
            "INFO:__main__:  Instantaneous batch size per device = 2\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 16\n",
            "INFO:__main__:  Total optimization steps = 8325\n",
            "Training...: 100% 925/925 [10:02<00:00,  1.54it/s]\n",
            "Epoch... (1/9 | Loss: 0.7702474594116211, Learning Rate: 0.00017780179041437805)\n",
            "Evaluating...:   0% 0/15 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 15/15 [01:33<00:00,  6.24s/it]\n",
            "Epoch... (1/9 | Eval Loss: 0.441 | Eval EM: 59.1748 | Eval F1: 76.4511 |)\n",
            "Training...: 100% 925/925 [08:27<00:00,  1.82it/s]\n",
            "Epoch... (2/9 | Loss: 0.9622405767440796, Learning Rate: 0.00015557957522105426)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.39it/s]\n",
            "Epoch... (2/9 | Eval Loss: 0.3885 | Eval EM: 64.4951 | Eval F1: 80.317 |)\n",
            "Training...: 100% 925/925 [08:22<00:00,  1.84it/s]\n",
            "Epoch... (3/9 | Loss: 0.41261494159698486, Learning Rate: 0.00013335736002773046)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.37it/s]\n",
            "Epoch... (3/9 | Eval Loss: 0.3719 | Eval EM: 66.2324 | Eval F1: 81.8435 |)\n",
            "Training...: 100% 925/925 [08:39<00:00,  1.78it/s]\n",
            "Epoch... (4/9 | Loss: 0.9402087330818176, Learning Rate: 0.00011113512300653383)\n",
            "Evaluating...: 100% 15/15 [00:11<00:00,  1.36it/s]\n",
            "Epoch... (4/9 | Eval Loss: 0.3313 | Eval EM: 66.2324 | Eval F1: 81.6282 |)\n",
            "Training...: 100% 925/925 [08:08<00:00,  1.89it/s]\n",
            "Epoch... (5/9 | Loss: 0.39733263850212097, Learning Rate: 8.891291508916765e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.40it/s]\n",
            "Epoch... (5/9 | Eval Loss: 0.3525 | Eval EM: 68.6211 | Eval F1: 82.3452 |)\n",
            "Training...: 100% 925/925 [08:28<00:00,  1.82it/s]\n",
            "Epoch... (6/9 | Loss: 0.4252464771270752, Learning Rate: 6.669067806797102e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.40it/s]\n",
            "Epoch... (6/9 | Eval Loss: 0.343 | Eval EM: 68.1868 | Eval F1: 82.999 |)\n",
            "Training...: 100% 925/925 [08:33<00:00,  1.80it/s]\n",
            "Epoch... (7/9 | Loss: 0.480455607175827, Learning Rate: 4.446846287464723e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.39it/s]\n",
            "Epoch... (7/9 | Eval Loss: 0.3534 | Eval EM: 69.7068 | Eval F1: 83.773 |)\n",
            "Training...: 100% 925/925 [08:31<00:00,  1.81it/s]\n",
            "Epoch... (8/9 | Loss: 0.1470717191696167, Learning Rate: 2.2246240405365825e-05)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.38it/s]\n",
            "Epoch... (8/9 | Eval Loss: 0.3568 | Eval EM: 69.3811 | Eval F1: 83.4893 |)\n",
            "Training...: 100% 925/925 [08:31<00:00,  1.81it/s]\n",
            "Epoch... (9/9 | Loss: 0.2359316051006317, Learning Rate: 2.4020671673952165e-08)\n",
            "Evaluating...: 100% 15/15 [00:10<00:00,  1.39it/s]\n",
            "Epoch... (9/9 | Eval Loss: 0.3621 | Eval EM: 70.7926 | Eval F1: 83.9824 |)\n",
            "Epoch... (8/9 | Eval Loss: 0.3568 | Eval EM: 69.3811 | Eval F1: 83.4893 |):  89% 8/9 [1:21:04<08:46, 526.51s/it]Configuration saved in /content/out/config.json\n",
            "Configuration saved in /content/out/generation_config.json\n",
            "Model weights saved in /content/out/flax_model.msgpack\n",
            "tokenizer config file saved in out/tokenizer_config.json\n",
            "Special tokens file saved in out/special_tokens_map.json\n",
            "Copy vocab file to out/spiece.model\n",
            "Epoch... (9/9 | Eval Loss: 0.3621 | Eval EM: 70.7926 | Eval F1: 83.9824 |): 100% 9/9 [1:21:10<00:00, 541.21s/it]\n"
          ]
        }
      ],
      "source": [
        "!python3 t5_qa.py --output_dir out --model_name_or_path sultan/ArabicT5-Base \\\n",
        "--tokenizer_name sultan/ArabicT5-Base \\\n",
        "--train_file=tydiqa-goldp-v1.1-train-pre.csv \\\n",
        "--validation_file=tydiqa-goldp-v1.1-dev-pre.csv \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--predict_with_generate \\\n",
        "--num_train_epochs 9 \\\n",
        "--overwrite_cache \\\n",
        "--learning_rate 2e-4 \\\n",
        "--warmup_steps 0 \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--overwrite_output_dir \\\n",
        "--max_source_length 512 \\\n",
        "--max_target_length 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8l6Uh0Hov5A"
      },
      "source": [
        "The code also will produce predictions and save it to \"pred.csv\" file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEvuvq7XTU8p"
      },
      "source": [
        "# **Running Flax Code on Text Summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IICQMDn6UUhS"
      },
      "source": [
        "Here we made small change to the Question Answering FLAX code in line 665 by having this code (two columns called \"text\" and \"summary\") should be in your dataset:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for text,summary in zip(examples[\"text\"],examples[\"summary\"]):\n",
        "            inputs.append(text)\n",
        "            targets.append(summary)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r2OA1IPAk2z"
      },
      "source": [
        "\n",
        "Please note that using the rouge package that comes with evaluate library will not properly compute the rouge score for Arabic, and the score will be very low, <10 for the RougeL score. This is because the raw rouge score will remove all non-English characters.\n",
        "\n",
        "You should also note that some papers use stemmers to calculate the rougeL score, and some don't. Xl-Sum dataset uses stemmer for the Arabic language, and we follow the same direction. Stemmer will increase the rougeL score by 4-5%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnT8LY1A59Xd",
        "outputId": "558437ac-a043-4f6c-c518-56c80847cf16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'xl-sum'...\n",
            "remote: Enumerating objects: 1159, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 1159 (delta 21), reused 30 (delta 12), pack-reused 1109\u001b[K\n",
            "Receiving objects: 100% (1159/1159), 5.53 MiB | 19.33 MiB/s, done.\n",
            "Resolving deltas: 100% (311/311), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/otuncelli/turkish-stemmer-python (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 1))\n",
            "  Cloning https://github.com/otuncelli/turkish-stemmer-python to /tmp/pip-req-build-6fi8vruq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/otuncelli/turkish-stemmer-python /tmp/pip-req-build-6fi8vruq\n",
            "  Resolved https://github.com/otuncelli/turkish-stemmer-python to commit 0c22380bf84a5ab1f219f4a905274c78afa04ed1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/abhik1505040/bengali-stemmer (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 2))\n",
            "  Cloning https://github.com/abhik1505040/bengali-stemmer to /tmp/pip-req-build-agjgcw5h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/abhik1505040/bengali-stemmer /tmp/pip-req-build-agjgcw5h\n",
            "  Resolved https://github.com/abhik1505040/bengali-stemmer to commit 375186caee8e50e3260dd6bc02d20d50277f3e39\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.10/dist-packages (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 6)) (1.16.0)\n",
            "Collecting pythainlp (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7))\n",
            "  Downloading pythainlp-4.0.2-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 8))\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 9)) (0.42.1)\n",
            "Collecting fugashi[unidic] (from -r xl-sum/multilingual_rouge_scoring/requirements.txt (line 10))\n",
            "  Downloading fugashi-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.9/599.9 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 4)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 4)) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7)) (2.27.1)\n",
            "Collecting unidic (from fugashi[unidic]->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 10))\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 7)) (3.4)\n",
            "Collecting wasabi<1.0.0,>=0.6.0 (from unidic->fugashi[unidic]->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 10))\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting plac<2.0.0,>=1.1.3 (from unidic->fugashi[unidic]->-r xl-sum/multilingual_rouge_scoring/requirements.txt (line 10))\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: TurkishStemmer, bengali-stemmer, unidic\n",
            "  Building wheel for TurkishStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TurkishStemmer: filename=TurkishStemmer-1.3-py3-none-any.whl size=19853 sha256=286184057a338ee4a86b59f68cd526ac8ef50ca9cef5b553eb9db112f16ed8a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tmpem0t0/wheels/6c/42/03/078ef17bc634a2d9c2f2312e7b737f6b91251e86fdbe10cdd7\n",
            "  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6391 sha256=20f8a4dd6e7e1c787d6856ed1fd5d26ae4f8ff0b4ff9c4d3bc983bc5b34187fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tmpem0t0/wheels/ae/f0/d6/e26e3828d716374e0664a2a5efcbb7b6f13df95f36e869089c\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7409 sha256=321fc01611af7a7da56058635f6bd59c7789f08aeac23d4a2ddc406f77b93008\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
            "Successfully built TurkishStemmer bengali-stemmer unidic\n",
            "Installing collected packages: wasabi, TurkishStemmer, plac, bengali-stemmer, pyonmttok, fugashi, unidic, pythainlp\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "Successfully installed TurkishStemmer-1.3 bengali-stemmer-0.0.1 fugashi-1.2.1 plac-1.3.5 pyonmttok-1.37.1 pythainlp-4.0.2 unidic-1.1.0 wasabi-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./xl-sum/multilingual_rouge_scoring\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.0.0) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.0.0) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.0.0) (1.24.3)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.0.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.0.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.0.0) (4.65.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.0.0-py3-none-any.whl size=18151 sha256=563054b94f032118a785506819bb80a8721cec88d28880f6fc85785a2eaa874b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wx2680m7/wheels/00/0f/35/562cf72e11a7e90b1e44d58da3254248df0e79eeebfa6eac49\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "  Attempting uninstall: rouge-score\n",
            "    Found existing installation: rouge-score 0.1.2\n",
            "    Uninstalling rouge-score-0.1.2:\n",
            "      Successfully uninstalled rouge-score-0.1.2\n",
            "Successfully installed rouge-score-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/csebuetnlp/xl-sum\n",
        "!pip3 install -r xl-sum/multilingual_rouge_scoring/requirements.txt\n",
        "!pip3 install  --upgrade xl-sum/multilingual_rouge_scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPBjklwt6cAP"
      },
      "source": [
        "We will evaluate our model on XL-Sum dataset. Download the Arabic portion from this link : https://github.com/csebuetnlp/xl-sum#datasets .\n",
        "\n",
        "After downloading it (arabic_XLSum_v2.0.tar.bz2) , upload it to this notebook and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZdmS6557c1U",
        "outputId": "365f912c-4030-4368-e548-78f3bde8bf46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./\n",
            "./arabic_test.jsonl\n",
            "./arabic_val.jsonl\n",
            "./arabic_train.jsonl\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf arabic_XLSum_v2.0.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4H5Ma15870n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "def convert_jsonl_to_csv(json_file,csv_file):\n",
        "  with open(csv_file,  'w', newline='') as f:\n",
        "    tsv_writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
        "    tsv_writer.writerow([\"id\",\"text\",\"title\",\"summary\"])\n",
        "    data = pd.read_json(json_file, lines=True,encoding='utf-8')\n",
        "    for index,row in data.iterrows():\n",
        "      tsv_writer.writerow([row[\"id\"],row[\"text\"],row[\"title\"],row[\"summary\"]])\n",
        "\n",
        "convert_jsonl_to_csv(\"arabic_train.jsonl\",\"XL-Sum-Train.csv\")\n",
        "convert_jsonl_to_csv(\"arabic_val.jsonl\",\"XL-Sum-Dev.csv\")\n",
        "convert_jsonl_to_csv(\"arabic_test.jsonl\",\"XL-Sum-Test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VqlSXVz-oWD"
      },
      "source": [
        "Lets see a sample from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVBrEruF-dId"
      },
      "outputs": [],
      "source": [
        "!head -100 XL-Sum-Train.csv >> XL-Sum-Train-Sample.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqN0AP1l-rZZ"
      },
      "source": [
        "Go to the left panel and look for the file XL-Sum-Train-Sample.csv and double-click on it to see the sample dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCYId6Qc3yD2"
      },
      "source": [
        "We need to download NLTK stopwords package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onaiJshC5Lx_",
        "outputId": "9f693b64-e607-47dc-dc3d-0342384d56d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1HkqzvT4FON"
      },
      "source": [
        "We updated our compute metric function to calcuate rouge score using this code :\n",
        "```\n",
        " scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True, lang=\"arabic\")\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    def Average(lst):\n",
        "        return sum(lst) / len(lst)\n",
        "    def calc_rougeL(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rougeL\"].fmeasure\n",
        "    def calc_rouge1(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rouge1\"].fmeasure\n",
        "    def calc_rouge2(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rouge2\"].fmeasure\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        rougeL_score_list=[]\n",
        "        rouge1_score_list=[]\n",
        "        rouge2_score_list=[]\n",
        "        for pred,label in zip(decoded_preds,decoded_labels):\n",
        "           rougeL_score_list.append(calc_rougeL(pred,label))\n",
        "           rouge1_score_list.append(calc_rouge1(pred,label))\n",
        "           rouge2_score_list.append(calc_rouge2(pred,label))\n",
        "        rougeL_score=Average(rougeL_score_list)\n",
        "        rouge1_score=Average(rouge1_score_list)\n",
        "        rouge2_score=Average(rouge2_score_list)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ELpooc4SQt"
      },
      "source": [
        "what we did here is that we compute the rouge scores for each entry, and then we took the average (mid) for all scores. This is in line with XL-Sum method to calculate the rouge score. Following the training process, we will verify this method using the official evaluation metric from Xl-Sum authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qfildV0TedE",
        "outputId": "54ed32fb-fbb7-4e3c-987e-cd49e56ff756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing t5_summary.py\n"
          ]
        }
      ],
      "source": [
        "#@title t5_summary.py\n",
        "%%writefile t5_summary.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from huggingface_hub import Repository\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "import wandb\n",
        "wandb.init(project=\"ArabicT5-Eval\")\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "logger = logging.getLogger(__name__)\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "    wandb.log({\"task_name\":\"Xl_Sum_Summary\",\"model_name\":model_args.model_name_or_path,\"per_device_train_batch_size\":training_args.per_device_train_batch_size, \"learning_rate\": training_args.learning_rate, \"weight_decay\":training_args.weight_decay, \"adam_beta1\":training_args.adam_beta1, \"adam_beta2\" : training_args.adam_beta2, \"adam_epsilon\":training_args.adam_epsilon,\"label_smoothing_factor\": training_args.label_smoothing_factor,\"num_train_epochs\":training_args.num_train_epochs, \"warmup_steps\":training_args.warmup_steps,\"seed\": training_args.seed})\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        #inputs = examples[\"context_question\"]\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "\n",
        "        for text,summary in zip(examples[\"text\"],examples[\"summary\"]):\n",
        "            inputs.append(text)\n",
        "            targets.append(summary)\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        if \"MBartModel\" in str(config.architectures):\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id)\n",
        "        else:\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    #metric = evaluate.load(\"rouge\")\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        labels = [label.strip() for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True, lang=\"arabic\")\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    def Average(lst):\n",
        "        return sum(lst) / len(lst)\n",
        "    def calc_rougeL(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rougeL\"].fmeasure\n",
        "    def calc_rouge1(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rouge1\"].fmeasure\n",
        "    def calc_rouge2(sent1,sent2):\n",
        "        scores = scorer.score(sent1,sent2)\n",
        "        return scores[\"rouge2\"].fmeasure\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        rougeL_score_list=[]\n",
        "        rouge1_score_list=[]\n",
        "        rouge2_score_list=[]\n",
        "        for pred,label in zip(decoded_preds,decoded_labels):\n",
        "           rougeL_score_list.append(calc_rougeL(pred,label))\n",
        "           rouge1_score_list.append(calc_rouge1(pred,label))\n",
        "           rouge2_score_list.append(calc_rouge2(pred,label))\n",
        "        rougeL_score=Average(rougeL_score_list)\n",
        "        rouge1_score=Average(rouge1_score_list)\n",
        "        rouge2_score=Average(rouge2_score_list)\n",
        "        #result = metric.compute(predictions=decoded_preds, references=decoded_labels,tokenizer=lambda x: x.split())\n",
        "        result={}\n",
        "        result[\"rouge1\"]=rouge1_score\n",
        "        result[\"rouge2\"]=rouge2_score\n",
        "        result[\"rougeL\"]=rougeL_score\n",
        "        result[\"bleu\"]=bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\",position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_log={\"Eval_\" + str(key): val for key, val in rouge_metrics.items()}\n",
        "            wandb.log(eval_log)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1 and 7==9:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            eval_log={\"Test_\" + str(key): val for key, val in rouge_metrics.items()}\n",
        "            wandb.log(eval_log)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq68drV42x6W",
        "outputId": "853c5f69-f34f-4ffc-d175-eafeae0dc18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-13 13:53:16.747333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, per_device_train_batch_size=3, per_device_eval_batch_size=3, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=11.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f5859a8dfb27cfd3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 8060.80it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1196.89it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f5859a8dfb27cfd3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 618.39it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 613/613 [00:00<00:00, 3.61MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Downloading spiece.model: 100% 924k/924k [00:00<00:00, 13.1MB/s]\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 653M/653M [00:09<00:00, 68.6MB/s]\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/dcc20c047baee12ba82586d6cdd47fee5c118533/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('lm_head', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('decoder', 'embed_tokens', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Downloading builder script: 100% 5.94k/5.94k [00:00<00:00, 2.64MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 2.02MB/s]       \n",
            "Downloading extra modules: 100% 3.34k/3.34k [00:00<00:00, 2.16MB/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 37519\n",
            "INFO:__main__:  Num Epochs = 11\n",
            "INFO:__main__:  Instantaneous batch size per device = 3\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 24\n",
            "INFO:__main__:  Total optimization steps = 17193\n",
            "Training...: 100% 1563/1563 [16:46<00:00,  1.55it/s]\n",
            "Epoch... (1/11 | Loss: 3.2551918029785156, Learning Rate: 0.00018182980420533568)\n",
            "Evaluating...:   0% 0/196 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 196/196 [02:12<00:00,  1.48it/s]\n",
            "Epoch... (1/11 | Eval Loss: 2.5957 | Eval rouge1: 31.0103 | Eval rouge2: 12.0865 | Eval rougeL: 25.8954 | Eval bleu: 6.2929 | Eval gen_len: 28.54681168692685 |)\n",
            "Training...: 100% 1563/1563 [13:57<00:00,  1.87it/s]\n",
            "Epoch... (2/11 | Loss: 2.720275402069092, Learning Rate: 0.00016364798648282886)\n",
            "Evaluating...: 100% 196/196 [00:48<00:00,  4.01it/s]\n",
            "Epoch... (2/11 | Eval Loss: 2.5413 | Eval rouge1: 32.2004 | Eval rouge2: 13.0712 | Eval rougeL: 26.8937 | Eval bleu: 7.0172 | Eval gen_len: 29.74429515888249 |)\n",
            "Training...: 100% 1563/1563 [15:22<00:00,  1.69it/s]\n",
            "Epoch... (3/11 | Loss: 3.083759069442749, Learning Rate: 0.00014546618331223726)\n",
            "Evaluating...: 100% 196/196 [00:45<00:00,  4.33it/s]\n",
            "Epoch... (3/11 | Eval Loss: 2.5184 | Eval rouge1: 33.3656 | Eval rouge2: 13.8264 | Eval rougeL: 27.9728 | Eval bleu: 7.1208 | Eval gen_len: 27.14587332053743 |)\n",
            "Training...: 100% 1563/1563 [17:11<00:00,  1.52it/s]\n",
            "Epoch... (4/11 | Loss: 2.3990509510040283, Learning Rate: 0.0001272843510378152)\n",
            "Evaluating...: 100% 196/196 [00:47<00:00,  4.10it/s]\n",
            "Epoch... (4/11 | Eval Loss: 2.5068 | Eval rouge1: 33.4716 | Eval rouge2: 14.003 | Eval rougeL: 28.0016 | Eval bleu: 7.3523 | Eval gen_len: 27.751332906803157 |)\n",
            "Training...: 100% 1563/1563 [17:32<00:00,  1.49it/s]\n",
            "Epoch... (5/11 | Loss: 2.5244829654693604, Learning Rate: 0.00010910253331530839)\n",
            "Evaluating...: 100% 196/196 [00:49<00:00,  3.93it/s]\n",
            "Epoch... (5/11 | Eval Loss: 2.5008 | Eval rouge1: 33.571 | Eval rouge2: 14.0589 | Eval rougeL: 28.1219 | Eval bleu: 7.477 | Eval gen_len: 27.801023672424822 |)\n",
            "Training...: 100% 1563/1563 [17:29<00:00,  1.49it/s]\n",
            "Epoch... (6/11 | Loss: 2.4537463188171387, Learning Rate: 9.092072286875919e-05)\n",
            "Evaluating...: 100% 196/196 [00:47<00:00,  4.11it/s]\n",
            "Epoch... (6/11 | Eval Loss: 2.4997 | Eval rouge1: 33.9685 | Eval rouge2: 14.351 | Eval rougeL: 28.5519 | Eval bleu: 7.5194 | Eval gen_len: 27.134357005758158 |)\n",
            "Training...: 100% 1563/1563 [17:25<00:00,  1.49it/s]\n",
            "Epoch... (7/11 | Loss: 2.5254342555999756, Learning Rate: 7.273890514625236e-05)\n",
            "Evaluating...: 100% 196/196 [00:44<00:00,  4.37it/s]\n",
            "Epoch... (7/11 | Eval Loss: 2.5121 | Eval rouge1: 34.2542 | Eval rouge2: 14.514 | Eval rougeL: 28.6224 | Eval bleu: 7.6213 | Eval gen_len: 27.056301983365323 |)\n",
            "Training...: 100% 1563/1563 [18:06<00:00,  1.44it/s]\n",
            "Epoch... (8/11 | Loss: 2.4358861446380615, Learning Rate: 5.4557083785766736e-05)\n",
            "Evaluating...: 100% 196/196 [00:47<00:00,  4.13it/s]\n",
            "Epoch... (8/11 | Eval Loss: 2.5166 | Eval rouge1: 34.4297 | Eval rouge2: 14.6644 | Eval rougeL: 28.9237 | Eval bleu: 7.6571 | Eval gen_len: 26.934527617828962 |)\n",
            "Training...: 100% 1563/1563 [18:21<00:00,  1.42it/s]\n",
            "Epoch... (9/11 | Loss: 2.175105571746826, Learning Rate: 3.637526970123872e-05)\n",
            "Evaluating...: 100% 196/196 [00:46<00:00,  4.19it/s]\n",
            "Epoch... (9/11 | Eval Loss: 2.5225 | Eval rouge1: 34.548 | Eval rouge2: 14.7675 | Eval rougeL: 28.9136 | Eval bleu: 7.7818 | Eval gen_len: 27.247387502665813 |)\n",
            "Training...: 100% 1563/1563 [18:47<00:00,  1.39it/s]\n",
            "Epoch... (10/11 | Loss: 2.532123565673828, Learning Rate: 1.8193459254689515e-05)\n",
            "Evaluating...: 100% 196/196 [00:45<00:00,  4.30it/s]\n",
            "Epoch... (10/11 | Eval Loss: 2.5291 | Eval rouge1: 34.597 | Eval rouge2: 14.7328 | Eval rougeL: 28.9642 | Eval bleu: 7.8327 | Eval gen_len: 27.37406696523779 |)\n",
            "Training...: 100% 1563/1563 [19:51<00:00,  1.31it/s]\n",
            "Epoch... (11/11 | Loss: 2.358738422393799, Learning Rate: 1.1634826435624745e-08)\n",
            "Evaluating...: 100% 196/196 [00:46<00:00,  4.23it/s]\n",
            "Epoch... (11/11 | Eval Loss: 2.5336 | Eval rouge1: 34.7863 | Eval rouge2: 14.8704 | Eval rougeL: 29.1368 | Eval bleu: 7.8373 | Eval gen_len: 27.178076348901683 |)\n",
            "Epoch... (11/11 | Eval Loss: 2.5336 | Eval rouge1: 34.7863 | Eval rouge2: 14.8704 | Eval rougeL: 29.1368 | Eval bleu: 7.8373 | Eval gen_len: 27.178076348901683 |): 100% 11/11 [3:26:11<00:00, 1124.66s/it]\n",
            "INFO:__main__:*** Predict ***\n",
            "Predicting...: 100% 196/196 [00:46<00:00,  4.24it/s]\n",
            "INFO:__main__:Predict Loss: 2.512944221496582 | Predict rouge1: 34.8084 | Predict rouge2: 14.8332 | Predict rougeL: 29.0797 | Predict bleu: 7.5588 | Predict gen_len: 27.30688846235871 |)\n"
          ]
        }
      ],
      "source": [
        "!python3 t5_summary.py --output_dir out \\\n",
        "--model_name_or_path sultan/ArabicT5-Base \\\n",
        "--tokenizer_name sultan/ArabicT5-Base \\\n",
        "--train_file=XL-Sum-Train.csv \\\n",
        "--validation_file=XL-Sum-Dev.csv \\\n",
        "--test_file=XL-Sum-Test.csv \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--predict_with_generate \\\n",
        "--num_train_epochs 11 \\\n",
        "--overwrite_cache \\\n",
        "--learning_rate 2e-4 \\\n",
        "--warmup_steps 0 \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 3 \\\n",
        "--overwrite_output_dir \\\n",
        "--max_source_length 512 \\\n",
        "--max_target_length 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Usf_to0IhCx"
      },
      "source": [
        "Let's now see our predictions (labels on the left and predictions on the right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_45XLTMFkn6N",
        "outputId": "aecc7802-ba89-4b21-9050-4ca2b7d521dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "أجلت السلطات العراقية أكثر من ألف مدني من القرى الموجودة على جبهة القتال الأمامية مع مواقع سيطرة تنظيم الدولة الإسلامية في معقله بمدينة الموصل. |||| قالت الأمم المتحدة إن نحو مليون شخص فروا من مدينة الموصل العراقية، التي يسيطر عليها مسلحو تنظيم الدولة الإسلامية.\n",
            "أدانت المكسيك القواعد الجديدة للهجرة التي أعلنت عنها الولايات المتحدة لترحيل المهاجرين حتى من هم غير المدانين. |||| أثارت الولايات المتحدة جدلا بشأن خطط أمريكية جديدة للهجرة، في وقت تشهد فيه البلاد جدلا بشأن خطط جديدة للهجرة.\n",
            "عرض المتحدث باسم جماعة أنصار الله الحوثية في اليمن ما قال إنه تفاصيل العملية العسكرية التي استهدفت قوات التحالف الذي تقوده السعودية وذلك في المنطقة الحدودية بين البلدين. |||| قال المتحدث باسم القوات المسلحة اليمنية إن القوات الحوثية قتلت أكثر من 100 من جنودها في عملية عسكرية واسعة بمنطقة نجران، جنوبي اليمن.\n",
            "استكمل الخبراء في مصر ترميم الكنيسة المعلقة في القاهرة بعد أعمال ترميم استغرقت 16 عاما. |||| افتتح رئيس الوزراء المصري، هشام قنديل، كنيسة جديدة في القاهرة، بعد ترميمها لمدة 14 عاما.\n",
            "يخشى خبراء في مجال الصحة من أن يكون شلل الأطفال قد عاد إلى سوريا التي مزقتها الحرب. |||| \"حذرت منظمة الصحة العالمية من أن سوريا تواجه \"\"خطرا كبيرا\"\" في انتشار مرض شلل الأطفال.\"\n",
            "مجموعة مختارة من أفضل الصور في القارة الأفريقية خلال الأسبوع الماضي، ومن أبرزها احتفالات أعياد الميلاد في مصر وأزمة الخبز في السودان وثلوج الجزائر. |||| تُظهر صور التقطت خلال الأيام القليلة الماضية صورا لأشخاص يُعتقد أنهم من ضحايا الفيضانات التي اجتاحت مختلف أنحاء العالم.\n",
            "استأنف الرئيس الفلسطيني محمود عباس ورئيس الوزراء الإسرائيلي بنيامين ننتياهو في واشنطن يوم الخميس مفاوضات السلام المباشرة. |||| بدأت في واشنطن أول محادثات مباشرة بين الإسرائيليين والفلسطينيين في واشنطن، وذلك بعد نحو عشرين شهرا من الحرب الأهلية التي استمرت نحو عشرين شهرا.\n",
            "هناك الكثير مما يجمع كارول نحاس بزوجها كارلوس غصن سواء على الصعيد الشخصي أم على الصعيد المهني، من بينها ريادة الأعمال والاغتراب وظروف الحياة. |||| في وقت سابق من العام الجاري، كانت كارول نحاس، زوجة كارلوس غصن، زوجة كارلوس غصن، قد تقدمت بشكوى ضد زوجها في اليابان، بعد أن اتهمته زوجته السابقة بسرقة سيارتها.\n",
            "افرجت السلطات السودانية عن 57 من معتقلي حركة العدل العدل والمساواة، إحدى أكبر حركات التمرد في اقليم دارفور بعد توصل الطرفين الثلاثاء الى اتفاق اطار يضع اسس المفاوضات لتسوية الصراع في الاقليم السوداني. |||| اعلنت حركة تحرير السودان المتمردة في اقليم دارفور السوداني اطلاق سراح اكثر من مئة من معتقليها في اطار اتفاق وقف اطلاق النار الذي ابرمته الحكومة السودانية مع الحكومة السودانية.\n",
            "قالت الحكومة الألمانية إن هناك تراجعا كبيرا في عدد المهاجرين الذين يصلون إليها لطلب اللجوء، وقال وزير الداخلية توماس دي ميزير إن عدد طلبات اللجوء انخفض بنسبة 66 بالمئة في مارس/آذار عن الشهر السابق لتصل إلى 20 ألفا. |||| قالت ألمانيا إنها ستسعى إلى تعزيز علاقاتها مع تركيا، وذلك بعد أن فشلت في التوصل إلى اتفاق مع تركيا بشأن المهاجرين.\n"
          ]
        }
      ],
      "source": [
        "!tail pred.csv | sed  's/,/ |||| /g'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy3k4J3FDhYk"
      },
      "source": [
        "Let's verify our rougeL score using an official evaluating script from Xl-Sum:\n",
        "\n",
        "https://github.com/csebuetnlp/xl-sum/tree/master/multilingual_rouge_scoring\n",
        "\n",
        "\n",
        "We will use our final prediction on the Test dataset, which our code stores in pred.csv.\n",
        "\n",
        "Our pred.csv will store predictions for both the dev and test datasets, but since our last evaluation was on the test dataset, it corresponds to the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgUGyqneDpxy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "predictions=[]\n",
        "references=[]\n",
        "pred=pd.read_csv(\"pred.csv\")\n",
        "with open('pred.txt', 'w', encoding='utf-8') as p:\n",
        "  with open('target.txt', 'w', encoding='utf-8') as t:\n",
        "   for index,row in pred.iterrows():\n",
        "    pred_value=row[\"pred\"].strip().replace('\\n', ' ').replace('\\r', '')\n",
        "    predictions.append(pred_value)\n",
        "    p.write(pred_value+\"\\n\")\n",
        "    label_value=row[\"label\"].strip().replace('\\n', ' ').replace('\\r', '')\n",
        "    references.append(label_value)\n",
        "    t.write(label_value+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPMC9fmzD1ly",
        "outputId": "e0aa8a9e-36b1-440e-9861-581d7d9cd411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I0613 17:42:17.584808 140595781957440 io.py:108] Reading targets from target.txt.\n",
            "I0613 17:42:17.585003 140595781957440 io.py:109] Reading predictions from pred.txt.\n",
            "I0613 17:42:24.185865 140595781957440 io.py:135] Writing results to scores.csv.\n",
            "I0613 17:42:24.186297 140595781957440 io.py:148] Finished writing results.\n"
          ]
        }
      ],
      "source": [
        "!python -m rouge_score.rouge --target_filepattern=target.txt \\\n",
        "--prediction_filepattern=pred.txt \\\n",
        "--output_filename=scores.csv \\\n",
        "--use_stemmer=True \\\n",
        "--lang=\"arabic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOWUfPw31bix",
        "outputId": "b5954b19-93cc-4ad4-8751-7011bf0e0fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_type,low,mid,high\n",
            "rouge1-F,0.344347,0.348071,0.352368\n",
            "rouge2-F,0.145052,0.148253,0.151825\n",
            "rougeL-F,0.287075,0.290823,0.294424\n"
          ]
        }
      ],
      "source": [
        "!cat /content/scores.csv | grep -E \"score|F\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvvryRI1i0i"
      },
      "source": [
        "compare F-scores (mid) from cell above with what we got from our predictions on the test dataset which is as follow :\n",
        "\n",
        "`INFO:__main__:Predict Loss: 2.512944221496582 | Predict rouge1: 34.8084 | Predict rouge2: 14.8332 | Predict rougeL: 29.0797 | Predict bleu: 7.5588 | Predict gen_len: 27.30688846235871 |)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8wvcn3bD2oP"
      },
      "source": [
        "without stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80z-Vr3KD56w",
        "outputId": "62f730a3-d045-4e52-bc3c-349beb3f959d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I0613 17:44:37.366029 140330213648192 io.py:108] Reading targets from target.txt.\n",
            "I0613 17:44:37.366205 140330213648192 io.py:109] Reading predictions from pred.txt.\n",
            "I0613 17:44:41.766485 140330213648192 io.py:135] Writing results to scores.csv.\n",
            "I0613 17:44:41.766892 140330213648192 io.py:148] Finished writing results.\n"
          ]
        }
      ],
      "source": [
        "!python -m rouge_score.rouge --target_filepattern=target.txt \\\n",
        "--prediction_filepattern=pred.txt \\\n",
        "--output_filename=scores.csv \\\n",
        "--use_stemmer=False \\\n",
        "--lang=\"arabic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZodpY5K3Di9",
        "outputId": "555aaf32-b462-49ed-8e70-2ad87dc0a3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_type,low,mid,high\n",
            "rouge1-F,0.282811,0.286766,0.290816\n",
            "rouge2-F,0.122022,0.125431,0.128796\n",
            "rougeL-F,0.244366,0.247818,0.251644\n"
          ]
        }
      ],
      "source": [
        "!cat /content/scores.csv | grep -E \"score|F\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YoRe3BHbZ2"
      },
      "source": [
        "If you want to disable stemmer in our T5 code, change the line 706 in t5_summary.py file:\n",
        "\n",
        "`scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=False, lang=\"arabic\")`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgv1CwNWYNvO"
      },
      "source": [
        "# **Running Flax Code on Text Classificaiton (Sentiment Analysis)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ6uxa8Df5yZ"
      },
      "source": [
        "In this example, we will finetune ArabicT5 on ArSarcasm-v2, which is part of WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. ArSarcasm-v2 has two tasks :\n",
        " * Sentimental Analysis (POS, NEG, NEU)\n",
        " * Scarcasm (True, False)\n",
        "\n",
        "Here we will focus on the Sentimental Analysis task. The official evaluation metric for the Sentimental Analysis task is F1 PN which only counts POS and NEG classes.\n",
        "\n",
        "Refer to this paper to get reported results of other Arabic NLP models:\n",
        "\n",
        "[Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection](https://aclanthology.org/2021.wanlp-1.3/).  Please note that seq2seq models like GPT-2, BART, and T5 are mainly designed to target generative tasks like Machine Translation, summarization, and Question Generation. However, here we extend the evaluation of T5 models to Text Classification tasks to show another case of T5 applications. Abstractive (extractive) language models like AraBERT, AraELECTRA, and our model [ArabicTransformer](https://aclanthology.org/2021.findings-emnlp.108/) should perform better in these tasks.\n",
        "\n",
        "\n",
        "Here we made a small change to the Question Answering FLAX code in line 665 by having this code :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for tweet,sentiment in zip(examples[\"tweet\"],examples[\"sentiment\"]):\n",
        "            inputs.append(tweet)\n",
        "            targets.append(sentiment)\n",
        "```\n",
        "and we also added a function to calculate F1 PN score for in line 603 :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def calc_scarcasm_score(y_pred,y_true):\n",
        "        accuracy=accuracy_score(y_true, y_pred)\n",
        "        f1_pn=f1_score(y_true, y_pred,labels=['negative','positive'],average=\"macro\")\n",
        "        return accuracy,f1_pn\n",
        "```\n",
        "\n",
        "\n",
        "and use this function in line 721\n",
        "\n",
        "```\n",
        "result={}\n",
        "accuracy,f1_pn=calc_scarcasm_score(decoded_preds,decoded_labels)\n",
        "result[\"Accuracy\"]=accuracy\n",
        "result[\"F1_PN\"]=f1_pn\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2CFl8IMYNvS",
        "outputId": "6c0c472c-0a73-4ea5-8c72-8e86f8111cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting t5_text_class.py\n"
          ]
        }
      ],
      "source": [
        "#@title T5 FLAX Text Classification\n",
        "%%writefile t5_text_class.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for summarization.\n",
        "\"\"\"\n",
        "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
        "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from jax.lib import xla_bridge\n",
        "import datasets\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from filelock import FileLock\n",
        "from flax import jax_utils, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSeq2SeqLM,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    label_smoothing_factor: float = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    eval_per_epoch: int = field(default=None, metadata={\"help\": \"Run an evaluation every X epochs\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "    gradient_checkpointing: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    dtype: Optional[str] = field(\n",
        "        default=\"float32\",\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
        "                \" `[float32, float16, bfloat16]`.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    text_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
        "    )\n",
        "    summary_column: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
        "                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n",
        "                \"during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    source_prefix: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
        "    )\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n",
        "                \"which is used during evaluation.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "summarization_name_mapping = {\n",
        "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
        "    \"big_patent\": (\"description\", \"abstract\"),\n",
        "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
        "    \"orange_sum\": (\"text\", \"summary\"),\n",
        "    \"pn_summary\": (\"article\", \"summary\"),\n",
        "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
        "    \"samsum\": (\"dialogue\", \"summary\"),\n",
        "    \"thaisum\": (\"body\", \"summary\"),\n",
        "    \"xglue\": (\"news_body\", \"news_title\"),\n",
        "    \"xsum\": (\"document\", \"summary\"),\n",
        "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
        "}\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jnp.ndarray\n",
        "\n",
        "    def replicate(self):\n",
        "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n",
        "\n",
        "\n",
        "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
        "    \"\"\"\n",
        "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
        "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "        batch_idx = np.asarray(batch_idx)\n",
        "    else:\n",
        "        batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    if drop_last:\n",
        "        steps_per_epoch = len(dataset) // batch_size\n",
        "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "    else:\n",
        "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n",
        "    summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "    train_metrics = get_metrics(train_metrics)\n",
        "    for key, vals in train_metrics.items():\n",
        "        tag = f\"train_{key}\"\n",
        "        for i, val in enumerate(vals):\n",
        "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    for metric_name, value in eval_metrics.items():\n",
        "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
        "    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
        "    #\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            keep_in_memory=False,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        dataset = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.tokenizer_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_fast=model_args.use_fast_tokenizer,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            from_pt=True,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        model = FlaxAutoModelForSeq2SeqLM.from_config(\n",
        "            config,\n",
        "            seed=training_args.seed,\n",
        "            dtype=getattr(jnp, model_args.dtype),\n",
        "        )\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "        model.enable_gradient_checkpointing()\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = dataset[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = dataset[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
        "    if data_args.text_column is None:\n",
        "        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        text_column = data_args.text_column\n",
        "        if text_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if data_args.summary_column is None:\n",
        "        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        summary_column = data_args.summary_column\n",
        "        if summary_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
        "    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
        "    # for that dynamically import the `shift_tokens_right` function from the model file\n",
        "    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n",
        "    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n",
        "\n",
        "    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n",
        "    def calc_scarcasm_score(y_pred,y_true):\n",
        "        accuracy=accuracy_score(y_true, y_pred)\n",
        "        f1_pn=f1_score(y_true, y_pred,labels=['negative','positive'],average=\"macro\")\n",
        "        return accuracy,f1_pn\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        inputs=[]\n",
        "        targets=[]\n",
        "\n",
        "        for tweet,sentiment in zip(examples[\"tweet\"],examples[\"sentiment\"]):\n",
        "            inputs.append(tweet)\n",
        "            targets.append(sentiment)\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        if \"MBartModel\" in str(config.architectures):\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id)\n",
        "        else:\n",
        "            decoder_input_ids = shift_tokens_right_fn(labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id)\n",
        "        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n",
        "\n",
        "        # We need decoder_attention_mask so we can ignore pad tokens from loss\n",
        "        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in dataset:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in dataset:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = dataset[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on validation dataset\",\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in dataset:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = dataset[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on prediction dataset\",\n",
        "        )\n",
        "\n",
        "    # Metric\n",
        "    def norm_seq(seq_text):\n",
        "        seq_text=seq_text.replace(\"<answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<nswer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"answer>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<itle>\",\"\")\n",
        "        seq_text=seq_text.replace(\"title>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<uestion>\",\"\")\n",
        "        seq_text=seq_text.replace(\"question>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<context>\",\"\")\n",
        "        seq_text=seq_text.replace(\"<ontext>\",\"\")\n",
        "        seq_text=seq_text.replace(\"context>\",\"\")\n",
        "        return seq_text\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [norm_seq(pred.strip()) for pred in preds]\n",
        "        labels = [norm_seq(label.strip()) for label in labels]\n",
        "\n",
        "        # rougeLSum expects newline after each sentence\n",
        "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(preds, labels):\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "        pred_dict=pd.DataFrame(columns=[\"label\",\"pred\"])\n",
        "        for label_a,pred_a in zip(decoded_labels,decoded_preds):\n",
        "         pred_dict=pd.concat([pred_dict, pd.DataFrame([{\"label\":label_a,\"pred\":pred_a}])], ignore_index=True)\n",
        "        pred_dict.to_csv(\"pred.csv\",index=False)\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result={}\n",
        "        accuracy,f1_pn=calc_scarcasm_score(decoded_preds,decoded_labels)\n",
        "        result[\"Accuracy\"]=accuracy\n",
        "        result[\"F1_PN\"]=f1_pn\n",
        "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "        return result\n",
        "\n",
        "    # Enable tensorboard only on the master node\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    # Initialize our training\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    rng, dropout_rng = jax.random.split(rng)\n",
        "\n",
        "    # Store some constant\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_train_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    # create adam optimizer\n",
        "    adamw = optax.adamw(\n",
        "        learning_rate=linear_decay_lr_schedule_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    # Setup train state\n",
        "    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
        "\n",
        "    # label smoothed cross entropy\n",
        "    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
        "        \"\"\"\n",
        "        The label smoothing implementation is adapted from Flax's official example:\n",
        "        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
        "        \"\"\"\n",
        "        vocab_size = logits.shape[-1]\n",
        "        confidence = 1.0 - label_smoothing_factor\n",
        "        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "        normalizing_constant = -(\n",
        "            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
        "        )\n",
        "        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "        loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
        "        loss = loss - normalizing_constant\n",
        "\n",
        "        # ignore padded tokens from loss\n",
        "        loss = loss * padding_mask\n",
        "        loss = loss.sum()\n",
        "        num_labels = padding_mask.sum()\n",
        "        return loss, num_labels\n",
        "\n",
        "    # Define gradient update step fn\n",
        "    def train_step(state, batch, label_smoothing_factor=0.0):\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
        "\n",
        "        def compute_loss(params):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "            return loss, num_labels\n",
        "\n",
        "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
        "        (loss, num_labels), grad = grad_fn(state.params)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        # true grad = total grad / total samples\n",
        "        grad = jax.lax.psum(grad, \"batch\")\n",
        "        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
        "        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n",
        "        return new_state, metrics\n",
        "\n",
        "    # Define eval fn\n",
        "    def eval_step(params, batch, label_smoothing_factor=0.0):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = model(**batch, params=params, train=False)[0]\n",
        "\n",
        "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
        "        num_labels = jax.lax.psum(num_labels, \"batch\")\n",
        "\n",
        "        # true loss = total loss / total samples\n",
        "        loss = jax.lax.psum(loss, \"batch\")\n",
        "        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
        "\n",
        "        metrics = {\"loss\": loss}\n",
        "        return metrics\n",
        "\n",
        "    # Define generation function\n",
        "    max_length = (\n",
        "        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n",
        "    )\n",
        "    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def generate_step(params, batch):\n",
        "        model.params = params\n",
        "        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n",
        "        return output_ids.sequences\n",
        "\n",
        "    # Create parallel version of the train and eval step\n",
        "    p_train_step = jax.pmap(\n",
        "        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
        "    )\n",
        "    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n",
        "    p_generate_step = jax.pmap(generate_step, \"batch\")\n",
        "\n",
        "    # Replicate the train state on each device\n",
        "    state = state.replicate()\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n",
        "\n",
        "    train_time = 0\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        # ======================== Training ================================\n",
        "        train_start = time.time()\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        train_metrics = []\n",
        "\n",
        "        # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
        "        steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "        # train\n",
        "        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=0, leave=True):\n",
        "            batch = next(train_loader)\n",
        "            batch = shard(batch)\n",
        "            state, train_metric = p_train_step(state, batch)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "        train_time += time.time() - train_start\n",
        "\n",
        "        train_metric = unreplicate(train_metric)\n",
        "\n",
        "        epochs.write(\n",
        "            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "            f\" {train_metric['learning_rate']})\"\n",
        "        )\n",
        "\n",
        "        # ======================== Evaluating ==============================\n",
        "        eval_metrics = []\n",
        "        eval_preds = []\n",
        "        eval_labels = []\n",
        "\n",
        "        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n",
        "        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(eval_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            eval_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                eval_labels.extend(labels)\n",
        "\n",
        "        # normalize eval metrics\n",
        "        eval_metrics = get_metrics(eval_metrics)\n",
        "        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n",
        "            eval_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics and update progress bar\n",
        "        loss_score=round(float(eval_metrics['loss']),4)\n",
        "        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {loss_score} | {rouge_desc})\"\n",
        "        epochs.write(desc)\n",
        "        epochs.desc = desc\n",
        "\n",
        "        # Save metrics\n",
        "        if has_tensorboard and jax.process_index() == 0:\n",
        "            cur_step = epoch * (len(train_dataset) // train_batch_size)\n",
        "            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n",
        "\n",
        "        # save checkpoint after each epoch and push checkpoint to the hub\n",
        "        if jax.process_index() == 0 and epoch == int(training_args.num_train_epochs)-1:\n",
        "            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n",
        "            model.save_pretrained(training_args.output_dir, params=params)\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            if training_args.push_to_hub:\n",
        "                repo.push_to_hub(commit_message=f\"Saving weights and logs of epoch {epoch}\", blocking=False)\n",
        "\n",
        "    # ======================== Prediction loop ==============================\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        pred_metrics = []\n",
        "        pred_generations = []\n",
        "        pred_labels = []\n",
        "\n",
        "        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n",
        "        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n",
        "        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=0, leave=True):\n",
        "            # Model forward\n",
        "            batch = next(pred_loader)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
        "                state.params, batch, min_device_batch=per_device_eval_batch_size\n",
        "            )\n",
        "            pred_metrics.append(metrics)\n",
        "\n",
        "            # generation\n",
        "            if data_args.predict_with_generate:\n",
        "                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
        "                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
        "                pred_labels.extend(labels)\n",
        "\n",
        "        # normalize prediction metrics\n",
        "        pred_metrics = get_metrics(pred_metrics)\n",
        "        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n",
        "\n",
        "        # compute ROUGE metrics\n",
        "        rouge_desc = \"\"\n",
        "        if data_args.predict_with_generate:\n",
        "            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n",
        "            pred_metrics.update(rouge_metrics)\n",
        "            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n",
        "\n",
        "        # Print metrics\n",
        "        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n",
        "        logger.info(desc)\n",
        "\n",
        "        # save final metrics in json\n",
        "        if jax.process_index() == 0:\n",
        "            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n",
        "            path = os.path.join(training_args.output_dir, \"test_results.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG0VrUmTYNvV",
        "outputId": "c19a3d7a-6469-4747-940f-d52c426b53aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-13 17:56:25--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585081 (571K) [text/plain]\n",
            "Saving to: ‘arsarcasm-v2_dev.csv’\n",
            "\n",
            "\rarsarcasm-v2_dev.cs   0%[                    ]       0  --.-KB/s               \rarsarcasm-v2_dev.cs 100%[===================>] 571.37K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-06-13 17:56:25 (13.5 MB/s) - ‘arsarcasm-v2_dev.csv’ saved [585081/585081]\n",
            "\n",
            "--2023-06-13 17:56:25--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393168 (2.3M) [text/plain]\n",
            "Saving to: ‘arsarcasm-v2_train.csv’\n",
            "\n",
            "arsarcasm-v2_train. 100%[===================>]   2.28M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-06-13 17:56:25 (32.9 MB/s) - ‘arsarcasm-v2_train.csv’ saved [2393168/2393168]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O arsarcasm-v2_dev.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
        "!wget -O arsarcasm-v2_train.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
        "'''\n",
        "we find an issue with T5 model when label are [\"POS\",\"NEG\",\"NEU\"] and model would truncate last letter so\n",
        "we will replace them to full words. This issue may caused by how our tokenizer handle non-arabic characters. We will fix this issue soon.\n",
        "This should not affect the evaluation or training on this task\n",
        "'''\n",
        "### we will\n",
        "!sed -i 's/POS/positive/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/NEG/negative/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/NEU/neutral/g' arsarcasm-v2_dev.csv\n",
        "!sed -i 's/POS/positive/g' arsarcasm-v2_train.csv\n",
        "!sed -i 's/NEG/negative/g' arsarcasm-v2_train.csv\n",
        "!sed -i 's/NEU/neutral/g' arsarcasm-v2_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWqoSxjIYNvV",
        "outputId": "1631ce94-df7c-4f5c-e0cc-899bde5ee641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-13 18:37:17.721959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='out', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, per_device_train_batch_size=2, per_device_eval_batch_size=4, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, label_smoothing_factor=0.0, adafactor=False, num_train_epochs=6.0, warmup_steps=0, logging_steps=500, save_steps=500, eval_steps=None, eval_per_epoch=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None, gradient_checkpointing=False)\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-81eba82563dc8d41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 2/2 [00:00<00:00, 725.85it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"sultan/ArabicT5-Base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 20,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 20,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--ArabicT5-Base/snapshots/3bb572922fd98ff6eee794b8d58da3fd5d310814/pytorch_model.bin\n",
            "PyTorch checkpoint contains 212,389,376 parameters.\n",
            "Some weights of the model checkpoint at sultan/ArabicT5-Base were not used when initializing FlaxT5ForConditionalGeneration: {('encoder', 'embed_tokens', 'kernel'), ('decoder', 'embed_tokens', 'kernel'), ('lm_head', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of FlaxT5ForConditionalGeneration were initialized from the model checkpoint at sultan/ArabicT5-Base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaxT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-81eba82563dc8d41/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b0f4cf3306876bf4.arrow\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 12548\n",
            "INFO:__main__:  Num Epochs = 6\n",
            "INFO:__main__:  Instantaneous batch size per device = 2\n",
            "INFO:__main__:  Total train batch size (w. parallel & distributed) = 16\n",
            "INFO:__main__:  Total optimization steps = 4704\n",
            "Training...: 100% 784/784 [08:16<00:00,  1.58it/s]\n",
            "Epoch... (1/6 | Loss: 0.17654308676719666, Learning Rate: 8.335459278896451e-05)\n",
            "Evaluating...:   0% 0/94 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Evaluating...: 100% 94/94 [01:23<00:00,  1.12it/s]\n",
            "Epoch... (1/6 | Eval Loss: 0.1977 | Eval Accuracy: 65.0667 | Eval F1_PN: 65.346 |)\n",
            "Training...: 100% 784/784 [05:51<00:00,  2.23it/s]\n",
            "Epoch... (2/6 | Loss: 0.20724733173847198, Learning Rate: 6.668792775599286e-05)\n",
            "Evaluating...: 100% 94/94 [00:05<00:00, 15.92it/s]\n",
            "Epoch... (2/6 | Eval Loss: 0.1978 | Eval Accuracy: 68.4333 | Eval F1_PN: 68.9313 |)\n",
            "Training...: 100% 784/784 [05:49<00:00,  2.24it/s]\n",
            "Epoch... (3/6 | Loss: 0.18203912675380707, Learning Rate: 5.00212590850424e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 15.02it/s]\n",
            "Epoch... (3/6 | Eval Loss: 0.1932 | Eval Accuracy: 69.1667 | Eval F1_PN: 71.0478 |)\n",
            "Training...: 100% 784/784 [05:39<00:00,  2.31it/s]\n",
            "Epoch... (4/6 | Loss: 0.12855182588100433, Learning Rate: 3.3354590414091945e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 14.74it/s]\n",
            "Epoch... (4/6 | Eval Loss: 0.1902 | Eval Accuracy: 68.9333 | Eval F1_PN: 70.2532 |)\n",
            "Training...: 100% 784/784 [05:43<00:00,  2.28it/s]\n",
            "Epoch... (5/6 | Loss: 0.10848049074411392, Learning Rate: 1.66879290190991e-05)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 13.82it/s]\n",
            "Epoch... (5/6 | Eval Loss: 0.1973 | Eval Accuracy: 69.3 | Eval F1_PN: 71.2392 |)\n",
            "Training...: 100% 784/784 [05:46<00:00,  2.26it/s]\n",
            "Epoch... (6/6 | Loss: 0.12080693244934082, Learning Rate: 2.1260976978965118e-08)\n",
            "Evaluating...: 100% 94/94 [00:06<00:00, 15.10it/s]\n",
            "Epoch... (6/6 | Eval Loss: 0.2028 | Eval Accuracy: 68.4333 | Eval F1_PN: 70.618 |)\n",
            "Epoch... (5/6 | Eval Loss: 0.1973 | Eval Accuracy: 69.3 | Eval F1_PN: 71.2392 |):  83% 5/6 [39:21<06:14, 374.38s/it]Configuration saved in /content/out/config.json\n",
            "Configuration saved in /content/out/generation_config.json\n",
            "Model weights saved in /content/out/flax_model.msgpack\n",
            "tokenizer config file saved in out/tokenizer_config.json\n",
            "Special tokens file saved in out/special_tokens_map.json\n",
            "Copy vocab file to out/spiece.model\n",
            "Epoch... (6/6 | Eval Loss: 0.2028 | Eval Accuracy: 68.4333 | Eval F1_PN: 70.618 |): 100% 6/6 [39:26<00:00, 394.46s/it]\n"
          ]
        }
      ],
      "source": [
        "!python3 t5_text_class.py --output_dir out --model_name_or_path sultan/ArabicT5-Base --tokenizer_name sultan/ArabicT5-Base --train_file=arsarcasm-v2_train.csv --validation_file=arsarcasm-v2_dev.csv --do_train --do_eval --predict_with_generate --num_train_epochs 6 --learning_rate 1e-4 --warmup_steps 0 --per_device_train_batch_size 2 --per_device_eval_batch_size 4 --overwrite_output_dir --max_source_length 128 --max_target_length 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VMqAH4NtqKE"
      },
      "source": [
        "refer to this paper for SOTA results on this task [Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection.](https://aclanthology.org/2021.wanlp-1.3/) <- Table 2 - Sentiment Analysis section (FPN score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}